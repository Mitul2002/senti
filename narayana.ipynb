{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "adb8a284-ff30-441a-89d0-a2937f6ece08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os \n",
    "\n",
    "\n",
    "os.chdir('C:\\\\Users\\\\MITUL\\\\Documents\\\\csv\\\\')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1bacded-e4eb-4814-b8ca-8bca86b1f7aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              review sentiment  \\\n",
      "0  One of the other reviewers has mentioned that ...  positive   \n",
      "1  A wonderful little production. <br /><br />The...  positive   \n",
      "2  I thought this was a wonderful way to spend ti...  positive   \n",
      "3  Basically there's a family where a little boy ...  negative   \n",
      "4  Petter Mattei's \"Love in the Time of Money\" is...  positive   \n",
      "\n",
      "                                              tokens  \\\n",
      "0  ['one', 'reviewer', 'mentioned', 'watching', '...   \n",
      "1  ['wonderful', 'little', 'production', 'filming...   \n",
      "2  ['thought', 'wonderful', 'way', 'spend', 'time...   \n",
      "3  ['basically', \"'s\", 'family', 'little', 'boy',...   \n",
      "4  ['petter', 'mattei', \"'s\", '``', 'love', 'time...   \n",
      "\n",
      "                                          doc_vector  \n",
      "0  [-1.51883094e-02  1.12578897e-02 -1.29181324e-...  \n",
      "1  [-5.40541969e-02  5.93277104e-02 -2.68855412e-...  \n",
      "2  [-3.38244177e-02 -8.09674710e-02  4.37558116e-...  \n",
      "3  [-4.89800125e-02 -1.27479985e-01  3.27471420e-...  \n",
      "4  [-6.47496060e-02 -6.59632012e-02 -3.40831950e-...  \n"
     ]
    }
   ],
   "source": [
    "# Load the IMDb dataset from a CSV file\n",
    "imdb_data = pd.read_csv('NLP/processed_imdb_data.csv')\n",
    "\n",
    "# Explore the dataset\n",
    "print(imdb_data.head())  # Display the first few rows of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1477dfc6-2881-4359-b6c7-84e533706666",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=imdb_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1609bf5-c39d-4e8d-9dbf-7d4a09836ef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longest Review:\n",
      "Match 1: Tag Team Table Match Bubba Ray and Spike Dudley vs Eddie Guerrero and Chris Benoit Bubba Ray and Spike Dudley started things off with a Tag Team Table Match against Eddie Guerrero and Chris Benoit. According to the rules of the match, both opponents have to go through tables in order to get the win. Benoit and Guerrero heated up early on by taking turns hammering first Spike and then Bubba Ray. A German suplex by Benoit to Bubba took the wind out of the Dudley brother. Spike tried to help his brother, but the referee restrained him while Benoit and Guerrero ganged up on him in the corner. With Benoit stomping away on Bubba, Guerrero set up a table outside. Spike dashed into the ring and somersaulted over the top rope onto Guerrero on the outside! After recovering and taking care of Spike, Guerrero slipped a table into the ring and helped the Wolverine set it up. The tandem then set up for a double superplex from the middle rope which would have put Bubba through the table, but Spike knocked the table over right before his brother came crashing down! Guerrero and Benoit propped another table in the corner and tried to Irish Whip Spike through it, but Bubba dashed in and blocked his brother. Bubba caught fire and lifted both opponents into back body drops! Bubba slammed Guerrero and Spike stomped on the Wolverine from off the top rope. Bubba held Benoit at bay for Spike to soar into the Wassup! headbutt! Shortly after, Benoit latched Spike in the Crossface, but the match continued even after Spike tapped out. Bubba came to his brother's rescue and managed to sprawl Benoit on a table. Bubba leapt from the middle rope, but Benoit moved and sent Bubba crashing through the wood! But because his opponents didn't force him through the table, Bubba was allowed to stay in the match. The first man was eliminated shortly after, though, as Spike put Eddie through a table with a Dudley Dawg from the ring apron to the outside! Benoit put Spike through a table moments later to even the score. Within seconds, Bubba nailed a Bubba Bomb that put Benoit through a table and gave the Dudleys the win! Winner: Bubba Ray and Spike Dudley<br /><br />Match 2: Cruiserweight Championship Jamie Noble vs Billy Kidman Billy Kidman challenged Jamie Noble, who brought Nidia with him to the ring, for the Cruiserweight Championship. Noble and Kidman locked up and tumbled over the ring, but raced back inside and grappled some more. When Kidman thwarted all Noble's moves, Noble fled outside the ring where Nidia gave him some encouragement. The fight spread outside the ring and Noble threw his girlfriend into the challenger. Kidman tossed Nidia aside but was taken down with a modified arm bar. Noble continued to attack Kidman's injured arm back in the ring. Kidman's injured harm hampered his offense, but he continued to battle hard. Noble tried to put Kidman away with a powerbomb but the challenger countered into a facebuster. Kidman went to finish things with a Shooting Star Press, but Noble broke up the attempt. Kidman went for the Shooting Star Press again, but this time Noble just rolled out of harm's way. Noble flipped Kidman into a power bomb soon after and got the pin to retain his WWE Cruiserweight Championship! Winner: Jamie Noble<br /><br />Match 3: European Championship William Regal vs Jeff Hardy William Regal took on Jeff Hardy next in an attempt to win back the European Championship. Jeff catapulted Regal over the top rope then took him down with a hurracanrana off the ring apron. Back in the ring, Jeff hit the Whisper in the wind to knock Regal for a loop. Jeff went for the Swanton Bomb, but Regal got his knees up to hit Jeff with a devastating shot. Jeff managed to surprise Regal with a quick rollup though and got the pin to keep the European Championship! Regal started bawling at seeing Hardy celebrate on his way back up the ramp. Winner: Jeff Hardy<br /><br />Match 4: Chris Jericho vs John Cena Chris Jericho had promised to end John Cena's career in their match at Vengeance, which came up next. Jericho tried to teach Cena a lesson as their match began by suplexing him to the mat. Jericho continued to knock Cena around the ring until his cockiness got the better of him. While on the top rope, Jericho began to showboat and allowed Cena to grab him for a superplex! Cena followed with a tilt-a-whirl slam but was taken down with a nasty dropkick to the gut. The rookie recovered and hit a belly to belly suplex but couldn't put Y2J away. Jericho launched into the Lionsault but Cena dodged the move. Jericho nailed a bulldog and then connected on the Lionsault, but did not go for the cover. He goaded Cena to his feet so he could put on the Walls of Jericho. Cena had other ideas, reversing the move into a pin attempt and getting the 1-2-3! Jericho went berserk after the match. Winner: John Cena<br /><br />Match 5: Intercontinental Championship RVD vs Brock Lesnar via disqualification The Next Big Thing and Mr. Pay-Per-View tangled with the Intercontinental Championship on the line. Brock grabbed the title from the ref and draped it over his shoulder momentarily while glaring at RVD. Van Dam 's quickness gave Brock fits early on. The big man rolled out of the ring and kicked the steel steps out of frustration. Brock pulled himself together and began to take charge. With Paul Heyman beaming at ringside, Brock slammed RVD to the hard floor outside the ring. From there, Brock began to overpower RVD, throwing him with ease over the top rope. RVD landed painfully on his back, then had to suffer from having his spine cracked against the steel ring steps. The fight returned to the ring with Brock squeezing RVD around the ribs. RVD broke away and soon after leveled Brock with a kick to the temple. RVD followed with the Rolling Thunder but Brock managed to kick out after a two-count. The fight looked like it might be over soon as RVD went for a Five-Star Frog Splash. Brock, though, hoisted Van Dam onto his shoulder and went for the F-5, but RVD whirled Brock into a DDT and followed with the Frog Splash! He went for the pin, but Heyman pulled the ref from the ring! The ref immediately called for a disqualification and soon traded blows with Heyman! After, RVD leapt onto Brock from the top rope and then threatened to hit the Van Terminator! Heyman grabbed RVD's leg and Brock picked up the champ and this time connected with the F-5 onto a steel chair! Winner: RVD<br /><br />Match 6: Booker T vs the Big Show Booker T faced the Big Show one-on-one next. Show withstood Booker T's kicks and punches and slapped Booker into the corner. After being thrown from the ring, Booker picked up a chair at ringside, but Big Show punched it back into Booker's face. Booker tried to get back into the game by choking Show with a camera cable at ringside. Booker smashed a TV monitor from the Spanish announcers' position into Show's skull, then delivered a scissors kick that put both men through the table! Booker crawled back into the ring and Big Show staggered in moments later. Show grabbed Booker's throat but was met by a low blow and a kick to the face. Booker climbed the top rope and nailed a somersaulting leg drop to get the pin! Winner: Booker T<br /><br />Announcement: Triple H entered the ring to a thunderous ovation as fans hoped to learn where The Game would end up competing. Before he could speak, Eric Bishoff stopped The Game to apologize for getting involved in his personal business. If Triple H signed with RAW, Bischoff promised his personal life would never come into play again. Bischoff said he's spent the past two years networking in Hollywood. He said everyone was looking for the next breakout WWE Superstar, and they were all talking about Triple H. Bischoff guaranteed that if Triple H signed with RAW, he'd be getting top opportunities coming his way. Stephanie McMahon stepped out to issue her own pitch. She said that because of her personal history with Triple H, the two of them know each other very well. She said the two of them were once unstoppable and they can be again. Bischoff cut her off and begged her to stop. Stephanie cited that Triple H once told her how Bischoff said Triple H had no talent and no charisma. Bischoff said he was young at the time and didn't know what he had, but he still has a lot more experience that Stephanie. The two continued to bicker back and forth, until Triple H stepped up with his microphone. The Game said it would be easy to say \"screw you\" to either one of them. Triple H went to shake Bischoff's hand, but pulled it away. He said he would rather go with the devil he knows, rather than the one he doesn't know. Before he could go any further, though, Shawn Michaels came out to shake things up. HBK said the last thing he wanted to do was cause any trouble. He didn't want to get involved, but he remembered pledging to bring Triple H to the nWo. HBK said there's nobody in the world that Triple H is better friends with. HBK told his friend to imagine the two back together again, making Bischoff's life a living hell. Triple H said that was a tempting offer. He then turned and hugged HBK, making official his switch to RAW! Triple H and HBK left, and Bischoff gloated over his victory. Bischoff said the difference between the two of them is that he's got testicles and she doesn't. Stephanie whacked Bischoff on the side of the head and left!<br /><br />Match 7: Tag Team Championship Match Christian and Lance Storm vs Hollywood Hogan and Edge The match started with loud \"USA\" chants and with Hogan shoving Christian through the ropes and out of the ring. The Canadians took over from there. But Edge scored a kick to Christian's head and planted a facebuster on Storm to get the tag to Hogan. Hogan began to Hulk up and soon caught Christian with a big boot and a leg drop! Storm broke up the count and Christian tossed Hogan from the ring where Storm superkicked the icon. Edge tagged in soon after and dropped both opponents. He speared both of them into the corner turnbuckles, but missed a spear on Strom and hit the ref hard instead. Edge nailed a DDT, but the ref was down and could not count. Test raced down and took down Hogan then leveled Edge with a boot. Storm tried to get the pin, but Edge kicked out after two. Riksihi sprinted in to fend off Test, allowing Edge to recover and spear Storm. Christian distracted the ref, though, and Y2J dashed in and clocked Edge with the Tag Team Championship! Storm rolled over and got the pinfall to win the title! Winners and New Tag Team Champions: Christian and Lance Storm<br /><br />Match 8: WWE Undisputed Championship Triple Threat Match. The Rock vs Kurt Angle and the Undertaker Three of WWE's most successful superstars lined up against each other in a Triple Threat Match with the Undisputed Championship hanging in the balance. Taker and The Rock got face to face with Kurt Angle begging for some attention off to the side. He got attention in the form of a beat down form the two other men. Soon after, Taker spilled out of the ring and The Rock brawled with Angle. Angle gave a series of suplexes that took down Rock, but the Great One countered with a DDT that managed a two-count. The fight continued outside the ring with Taker coming to life and clotheslining Angle and repeatedly smacking The Rock. Taker and Rock got into it back into the ring, and Taker dropped The Rock with a sidewalk slam to get a two-count. Rock rebounded, grabbed Taker by the throat and chokeslammed him! Angle broke up the pin attempt that likely would have given The Rock the title. The Rock retaliated by latching on the ankle lock to Kurt Angle. Angle reversed the move and Rock Bottomed the People's Champion. Soon after, The Rock disposed of Angle and hit the People's Elbow on the Undertaker. Angle tried to take advantage by disabling the Great One outside the ring and covering Taker, who kicked out after a two count. Outside the ring, Rock took a big swig from a nearby water bottle and spewed the liquid into Taker's face to blind the champion. Taker didn't stay disabled for long, and managed to overpower Rock and turn his attention to Angle. Taker landed a guillotine leg drop onto Angle, laying on the ring apron. The Rock picked himself up just in time to break up a pin attempt on Kurt Angle. Taker nailed Rock with a DDT and set him up for a chokeslam. ANgle tried sneaking up with a steel chair, but Taker caught on to that tomfoolery and smacked it out of his hands. The referee got caught in the ensuing fire and didn't see Angle knock Taker silly with a steel chair. Angle went to cover Taker as The Rock lay prone, but the Dead Man somehow got his shoulder up. Angle tried to pin Rock, but he too kicked out. The Rock got up and landed Angle in the sharpshooter! Angle looked like he was about to tap, but Taker kicked The Rock out of the submission hold. Taker picked Rock up and crashed him with the Last Ride. While the Dead Man covered him for the win, Angle raced in and picked Taker up in the ankle lock! Taker went delirious with pain, but managed to counter. He picked Angle up for the last ride, but Angle put on a triangle choke! It looked like Taker was about to pass out, but The Rock broke Angle's hold only to find himself caught in the ankle lock. Rock got out of the hold and watched Taker chokeslam Angle. Rocky hit the Rock Bottom, but Taker refused to go down and kicked out. Angle whirled Taker up into the Angle Slam but was Rock Bottomed by the Great One and pinned! Winner and New WWE Champion: The Rock<br /><br />~Finally there is a decent PPV! Lately the PPV weren't very good, but this one was a winner. I give this PPV a A-<br /><br />\n"
     ]
    }
   ],
   "source": [
    "index_of_longest_review = df['review'].str.len().idxmax()\n",
    "\n",
    "# Get the longest review\n",
    "longest_review = df.loc[index_of_longest_review, 'review']\n",
    "\n",
    "# Display the longest review\n",
    "print(\"Longest Review:\")\n",
    "print(longest_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b74fecbb-f157-4003-90cc-070de7b10974",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "import spacy\n",
    "import pandas as pd\n",
    "\n",
    "# Download necessary NLTK datasets\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "153f438f-9bbd-4c06-be05-631d5bb7f940",
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_data['tokens'] = imdb_data['review'].apply(word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "82313fe0-afe8-4fae-b110-bcfd0caead8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 1 Tokens:\n",
      "['One', 'of', 'the', 'other', 'reviewers', 'has', 'mentioned', 'that', 'after', 'watching', 'just', '1', 'Oz', 'episode', 'you', \"'ll\", 'be', 'hooked', '.', 'They', 'are', 'right', ',', 'as', 'this', 'is', 'exactly', 'what', 'happened', 'with', 'me.', '<', 'br', '/', '>', '<', 'br', '/', '>', 'The', 'first', 'thing', 'that', 'struck', 'me', 'about', 'Oz', 'was', 'its', 'brutality', 'and', 'unflinching', 'scenes', 'of', 'violence', ',', 'which', 'set', 'in', 'right', 'from', 'the', 'word', 'GO', '.', 'Trust', 'me', ',', 'this', 'is', 'not', 'a', 'show', 'for', 'the', 'faint', 'hearted', 'or', 'timid', '.', 'This', 'show', 'pulls', 'no', 'punches', 'with', 'regards', 'to', 'drugs', ',', 'sex', 'or', 'violence', '.', 'Its', 'is', 'hardcore', ',', 'in', 'the', 'classic', 'use', 'of', 'the', 'word.', '<', 'br', '/', '>', '<', 'br', '/', '>', 'It', 'is', 'called', 'OZ', 'as', 'that', 'is', 'the', 'nickname', 'given', 'to', 'the', 'Oswald', 'Maximum', 'Security', 'State', 'Penitentary', '.', 'It', 'focuses', 'mainly', 'on', 'Emerald', 'City', ',', 'an', 'experimental', 'section', 'of', 'the', 'prison', 'where', 'all', 'the', 'cells', 'have', 'glass', 'fronts', 'and', 'face', 'inwards', ',', 'so', 'privacy', 'is', 'not', 'high', 'on', 'the', 'agenda', '.', 'Em', 'City', 'is', 'home', 'to', 'many', '..', 'Aryans', ',', 'Muslims', ',', 'gangstas', ',', 'Latinos', ',', 'Christians', ',', 'Italians', ',', 'Irish', 'and', 'more', '....', 'so', 'scuffles', ',', 'death', 'stares', ',', 'dodgy', 'dealings', 'and', 'shady', 'agreements', 'are', 'never', 'far', 'away.', '<', 'br', '/', '>', '<', 'br', '/', '>', 'I', 'would', 'say', 'the', 'main', 'appeal', 'of', 'the', 'show', 'is', 'due', 'to', 'the', 'fact', 'that', 'it', 'goes', 'where', 'other', 'shows', 'would', \"n't\", 'dare', '.', 'Forget', 'pretty', 'pictures', 'painted', 'for', 'mainstream', 'audiences', ',', 'forget', 'charm', ',', 'forget', 'romance', '...', 'OZ', 'does', \"n't\", 'mess', 'around', '.', 'The', 'first', 'episode', 'I', 'ever', 'saw', 'struck', 'me', 'as', 'so', 'nasty', 'it', 'was', 'surreal', ',', 'I', 'could', \"n't\", 'say', 'I', 'was', 'ready', 'for', 'it', ',', 'but', 'as', 'I', 'watched', 'more', ',', 'I', 'developed', 'a', 'taste', 'for', 'Oz', ',', 'and', 'got', 'accustomed', 'to', 'the', 'high', 'levels', 'of', 'graphic', 'violence', '.', 'Not', 'just', 'violence', ',', 'but', 'injustice', '(', 'crooked', 'guards', 'who', \"'ll\", 'be', 'sold', 'out', 'for', 'a', 'nickel', ',', 'inmates', 'who', \"'ll\", 'kill', 'on', 'order', 'and', 'get', 'away', 'with', 'it', ',', 'well', 'mannered', ',', 'middle', 'class', 'inmates', 'being', 'turned', 'into', 'prison', 'bitches', 'due', 'to', 'their', 'lack', 'of', 'street', 'skills', 'or', 'prison', 'experience', ')', 'Watching', 'Oz', ',', 'you', 'may', 'become', 'comfortable', 'with', 'what', 'is', 'uncomfortable', 'viewing', '....', 'thats', 'if', 'you', 'can', 'get', 'in', 'touch', 'with', 'your', 'darker', 'side', '.']\n",
      "\n",
      "Review 2 Tokens:\n",
      "['A', 'wonderful', 'little', 'production', '.', '<', 'br', '/', '>', '<', 'br', '/', '>', 'The', 'filming', 'technique', 'is', 'very', 'unassuming-', 'very', 'old-time-BBC', 'fashion', 'and', 'gives', 'a', 'comforting', ',', 'and', 'sometimes', 'discomforting', ',', 'sense', 'of', 'realism', 'to', 'the', 'entire', 'piece', '.', '<', 'br', '/', '>', '<', 'br', '/', '>', 'The', 'actors', 'are', 'extremely', 'well', 'chosen-', 'Michael', 'Sheen', 'not', 'only', '``', 'has', 'got', 'all', 'the', 'polari', \"''\", 'but', 'he', 'has', 'all', 'the', 'voices', 'down', 'pat', 'too', '!', 'You', 'can', 'truly', 'see', 'the', 'seamless', 'editing', 'guided', 'by', 'the', 'references', 'to', 'Williams', \"'\", 'diary', 'entries', ',', 'not', 'only', 'is', 'it', 'well', 'worth', 'the', 'watching', 'but', 'it', 'is', 'a', 'terrificly', 'written', 'and', 'performed', 'piece', '.', 'A', 'masterful', 'production', 'about', 'one', 'of', 'the', 'great', 'master', \"'s\", 'of', 'comedy', 'and', 'his', 'life', '.', '<', 'br', '/', '>', '<', 'br', '/', '>', 'The', 'realism', 'really', 'comes', 'home', 'with', 'the', 'little', 'things', ':', 'the', 'fantasy', 'of', 'the', 'guard', 'which', ',', 'rather', 'than', 'use', 'the', 'traditional', \"'dream\", \"'\", 'techniques', 'remains', 'solid', 'then', 'disappears', '.', 'It', 'plays', 'on', 'our', 'knowledge', 'and', 'our', 'senses', ',', 'particularly', 'with', 'the', 'scenes', 'concerning', 'Orton', 'and', 'Halliwell', 'and', 'the', 'sets', '(', 'particularly', 'of', 'their', 'flat', 'with', 'Halliwell', \"'s\", 'murals', 'decorating', 'every', 'surface', ')', 'are', 'terribly', 'well', 'done', '.']\n",
      "\n",
      "Review 3 Tokens:\n",
      "['I', 'thought', 'this', 'was', 'a', 'wonderful', 'way', 'to', 'spend', 'time', 'on', 'a', 'too', 'hot', 'summer', 'weekend', ',', 'sitting', 'in', 'the', 'air', 'conditioned', 'theater', 'and', 'watching', 'a', 'light-hearted', 'comedy', '.', 'The', 'plot', 'is', 'simplistic', ',', 'but', 'the', 'dialogue', 'is', 'witty', 'and', 'the', 'characters', 'are', 'likable', '(', 'even', 'the', 'well', 'bread', 'suspected', 'serial', 'killer', ')', '.', 'While', 'some', 'may', 'be', 'disappointed', 'when', 'they', 'realize', 'this', 'is', 'not', 'Match', 'Point', '2', ':', 'Risk', 'Addiction', ',', 'I', 'thought', 'it', 'was', 'proof', 'that', 'Woody', 'Allen', 'is', 'still', 'fully', 'in', 'control', 'of', 'the', 'style', 'many', 'of', 'us', 'have', 'grown', 'to', 'love.', '<', 'br', '/', '>', '<', 'br', '/', '>', 'This', 'was', 'the', 'most', 'I', \"'d\", 'laughed', 'at', 'one', 'of', 'Woody', \"'s\", 'comedies', 'in', 'years', '(', 'dare', 'I', 'say', 'a', 'decade', '?', ')', '.', 'While', 'I', \"'ve\", 'never', 'been', 'impressed', 'with', 'Scarlet', 'Johanson', ',', 'in', 'this', 'she', 'managed', 'to', 'tone', 'down', 'her', '``', 'sexy', \"''\", 'image', 'and', 'jumped', 'right', 'into', 'a', 'average', ',', 'but', 'spirited', 'young', 'woman.', '<', 'br', '/', '>', '<', 'br', '/', '>', 'This', 'may', 'not', 'be', 'the', 'crown', 'jewel', 'of', 'his', 'career', ',', 'but', 'it', 'was', 'wittier', 'than', '``', 'Devil', 'Wears', 'Prada', \"''\", 'and', 'more', 'interesting', 'than', '``', 'Superman', \"''\", 'a', 'great', 'comedy', 'to', 'go', 'see', 'with', 'friends', '.']\n",
      "\n",
      "Review 4 Tokens:\n",
      "['Basically', 'there', \"'s\", 'a', 'family', 'where', 'a', 'little', 'boy', '(', 'Jake', ')', 'thinks', 'there', \"'s\", 'a', 'zombie', 'in', 'his', 'closet', '&', 'his', 'parents', 'are', 'fighting', 'all', 'the', 'time.', '<', 'br', '/', '>', '<', 'br', '/', '>', 'This', 'movie', 'is', 'slower', 'than', 'a', 'soap', 'opera', '...', 'and', 'suddenly', ',', 'Jake', 'decides', 'to', 'become', 'Rambo', 'and', 'kill', 'the', 'zombie.', '<', 'br', '/', '>', '<', 'br', '/', '>', 'OK', ',', 'first', 'of', 'all', 'when', 'you', \"'re\", 'going', 'to', 'make', 'a', 'film', 'you', 'must', 'Decide', 'if', 'its', 'a', 'thriller', 'or', 'a', 'drama', '!', 'As', 'a', 'drama', 'the', 'movie', 'is', 'watchable', '.', 'Parents', 'are', 'divorcing', '&', 'arguing', 'like', 'in', 'real', 'life', '.', 'And', 'then', 'we', 'have', 'Jake', 'with', 'his', 'closet', 'which', 'totally', 'ruins', 'all', 'the', 'film', '!', 'I', 'expected', 'to', 'see', 'a', 'BOOGEYMAN', 'similar', 'movie', ',', 'and', 'instead', 'i', 'watched', 'a', 'drama', 'with', 'some', 'meaningless', 'thriller', 'spots.', '<', 'br', '/', '>', '<', 'br', '/', '>', '3', 'out', 'of', '10', 'just', 'for', 'the', 'well', 'playing', 'parents', '&', 'descent', 'dialogs', '.', 'As', 'for', 'the', 'shots', 'with', 'Jake', ':', 'just', 'ignore', 'them', '.']\n",
      "\n",
      "Review 5 Tokens:\n",
      "['Petter', 'Mattei', \"'s\", '``', 'Love', 'in', 'the', 'Time', 'of', 'Money', \"''\", 'is', 'a', 'visually', 'stunning', 'film', 'to', 'watch', '.', 'Mr.', 'Mattei', 'offers', 'us', 'a', 'vivid', 'portrait', 'about', 'human', 'relations', '.', 'This', 'is', 'a', 'movie', 'that', 'seems', 'to', 'be', 'telling', 'us', 'what', 'money', ',', 'power', 'and', 'success', 'do', 'to', 'people', 'in', 'the', 'different', 'situations', 'we', 'encounter', '.', '<', 'br', '/', '>', '<', 'br', '/', '>', 'This', 'being', 'a', 'variation', 'on', 'the', 'Arthur', 'Schnitzler', \"'s\", 'play', 'about', 'the', 'same', 'theme', ',', 'the', 'director', 'transfers', 'the', 'action', 'to', 'the', 'present', 'time', 'New', 'York', 'where', 'all', 'these', 'different', 'characters', 'meet', 'and', 'connect', '.', 'Each', 'one', 'is', 'connected', 'in', 'one', 'way', ',', 'or', 'another', 'to', 'the', 'next', 'person', ',', 'but', 'no', 'one', 'seems', 'to', 'know', 'the', 'previous', 'point', 'of', 'contact', '.', 'Stylishly', ',', 'the', 'film', 'has', 'a', 'sophisticated', 'luxurious', 'look', '.', 'We', 'are', 'taken', 'to', 'see', 'how', 'these', 'people', 'live', 'and', 'the', 'world', 'they', 'live', 'in', 'their', 'own', 'habitat.', '<', 'br', '/', '>', '<', 'br', '/', '>', 'The', 'only', 'thing', 'one', 'gets', 'out', 'of', 'all', 'these', 'souls', 'in', 'the', 'picture', 'is', 'the', 'different', 'stages', 'of', 'loneliness', 'each', 'one', 'inhabits', '.', 'A', 'big', 'city', 'is', 'not', 'exactly', 'the', 'best', 'place', 'in', 'which', 'human', 'relations', 'find', 'sincere', 'fulfillment', ',', 'as', 'one', 'discerns', 'is', 'the', 'case', 'with', 'most', 'of', 'the', 'people', 'we', 'encounter.', '<', 'br', '/', '>', '<', 'br', '/', '>', 'The', 'acting', 'is', 'good', 'under', 'Mr.', 'Mattei', \"'s\", 'direction', '.', 'Steve', 'Buscemi', ',', 'Rosario', 'Dawson', ',', 'Carol', 'Kane', ',', 'Michael', 'Imperioli', ',', 'Adrian', 'Grenier', ',', 'and', 'the', 'rest', 'of', 'the', 'talented', 'cast', ',', 'make', 'these', 'characters', 'come', 'alive.', '<', 'br', '/', '>', '<', 'br', '/', '>', 'We', 'wish', 'Mr.', 'Mattei', 'good', 'luck', 'and', 'await', 'anxiously', 'for', 'his', 'next', 'work', '.']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(f\"Review {i+1} Tokens:\")\n",
    "    print(imdb_data['tokens'][i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cd81652e-1fb8-41ad-8136-d5a2b636d710",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove tokens containing '<' or '>'\n",
    "def remove_angle_brackets(tokens):\n",
    "    return [token for token in tokens if '<' not in token and '>' not in token]\n",
    "\n",
    "# Apply the function to your 'tokens' column\n",
    "imdb_data['tokens'] = imdb_data['tokens'].apply(remove_angle_brackets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e48267f2-4d9b-49f0-a36a-95b9de1d9378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 27 Tokens:\n",
      "['One', 'of', 'the', 'other', 'reviewers', 'has', 'mentioned', 'that', 'after', 'watching', 'just', '1', 'Oz', 'episode', 'you', \"'ll\", 'be', 'hooked', '.', 'They', 'are', 'right', ',', 'as', 'this', 'is', 'exactly', 'what', 'happened', 'with', 'me.', 'br', '/', 'br', '/', 'The', 'first', 'thing', 'that', 'struck', 'me', 'about', 'Oz', 'was', 'its', 'brutality', 'and', 'unflinching', 'scenes', 'of', 'violence', ',', 'which', 'set', 'in', 'right', 'from', 'the', 'word', 'GO', '.', 'Trust', 'me', ',', 'this', 'is', 'not', 'a', 'show', 'for', 'the', 'faint', 'hearted', 'or', 'timid', '.', 'This', 'show', 'pulls', 'no', 'punches', 'with', 'regards', 'to', 'drugs', ',', 'sex', 'or', 'violence', '.', 'Its', 'is', 'hardcore', ',', 'in', 'the', 'classic', 'use', 'of', 'the', 'word.', 'br', '/', 'br', '/', 'It', 'is', 'called', 'OZ', 'as', 'that', 'is', 'the', 'nickname', 'given', 'to', 'the', 'Oswald', 'Maximum', 'Security', 'State', 'Penitentary', '.', 'It', 'focuses', 'mainly', 'on', 'Emerald', 'City', ',', 'an', 'experimental', 'section', 'of', 'the', 'prison', 'where', 'all', 'the', 'cells', 'have', 'glass', 'fronts', 'and', 'face', 'inwards', ',', 'so', 'privacy', 'is', 'not', 'high', 'on', 'the', 'agenda', '.', 'Em', 'City', 'is', 'home', 'to', 'many', '..', 'Aryans', ',', 'Muslims', ',', 'gangstas', ',', 'Latinos', ',', 'Christians', ',', 'Italians', ',', 'Irish', 'and', 'more', '....', 'so', 'scuffles', ',', 'death', 'stares', ',', 'dodgy', 'dealings', 'and', 'shady', 'agreements', 'are', 'never', 'far', 'away.', 'br', '/', 'br', '/', 'I', 'would', 'say', 'the', 'main', 'appeal', 'of', 'the', 'show', 'is', 'due', 'to', 'the', 'fact', 'that', 'it', 'goes', 'where', 'other', 'shows', 'would', \"n't\", 'dare', '.', 'Forget', 'pretty', 'pictures', 'painted', 'for', 'mainstream', 'audiences', ',', 'forget', 'charm', ',', 'forget', 'romance', '...', 'OZ', 'does', \"n't\", 'mess', 'around', '.', 'The', 'first', 'episode', 'I', 'ever', 'saw', 'struck', 'me', 'as', 'so', 'nasty', 'it', 'was', 'surreal', ',', 'I', 'could', \"n't\", 'say', 'I', 'was', 'ready', 'for', 'it', ',', 'but', 'as', 'I', 'watched', 'more', ',', 'I', 'developed', 'a', 'taste', 'for', 'Oz', ',', 'and', 'got', 'accustomed', 'to', 'the', 'high', 'levels', 'of', 'graphic', 'violence', '.', 'Not', 'just', 'violence', ',', 'but', 'injustice', '(', 'crooked', 'guards', 'who', \"'ll\", 'be', 'sold', 'out', 'for', 'a', 'nickel', ',', 'inmates', 'who', \"'ll\", 'kill', 'on', 'order', 'and', 'get', 'away', 'with', 'it', ',', 'well', 'mannered', ',', 'middle', 'class', 'inmates', 'being', 'turned', 'into', 'prison', 'bitches', 'due', 'to', 'their', 'lack', 'of', 'street', 'skills', 'or', 'prison', 'experience', ')', 'Watching', 'Oz', ',', 'you', 'may', 'become', 'comfortable', 'with', 'what', 'is', 'uncomfortable', 'viewing', '....', 'thats', 'if', 'you', 'can', 'get', 'in', 'touch', 'with', 'your', 'darker', 'side', '.']\n",
      "\n",
      "Review 28 Tokens:\n",
      "['A', 'wonderful', 'little', 'production', '.', 'br', '/', 'br', '/', 'The', 'filming', 'technique', 'is', 'very', 'unassuming-', 'very', 'old-time-BBC', 'fashion', 'and', 'gives', 'a', 'comforting', ',', 'and', 'sometimes', 'discomforting', ',', 'sense', 'of', 'realism', 'to', 'the', 'entire', 'piece', '.', 'br', '/', 'br', '/', 'The', 'actors', 'are', 'extremely', 'well', 'chosen-', 'Michael', 'Sheen', 'not', 'only', '``', 'has', 'got', 'all', 'the', 'polari', \"''\", 'but', 'he', 'has', 'all', 'the', 'voices', 'down', 'pat', 'too', '!', 'You', 'can', 'truly', 'see', 'the', 'seamless', 'editing', 'guided', 'by', 'the', 'references', 'to', 'Williams', \"'\", 'diary', 'entries', ',', 'not', 'only', 'is', 'it', 'well', 'worth', 'the', 'watching', 'but', 'it', 'is', 'a', 'terrificly', 'written', 'and', 'performed', 'piece', '.', 'A', 'masterful', 'production', 'about', 'one', 'of', 'the', 'great', 'master', \"'s\", 'of', 'comedy', 'and', 'his', 'life', '.', 'br', '/', 'br', '/', 'The', 'realism', 'really', 'comes', 'home', 'with', 'the', 'little', 'things', ':', 'the', 'fantasy', 'of', 'the', 'guard', 'which', ',', 'rather', 'than', 'use', 'the', 'traditional', \"'dream\", \"'\", 'techniques', 'remains', 'solid', 'then', 'disappears', '.', 'It', 'plays', 'on', 'our', 'knowledge', 'and', 'our', 'senses', ',', 'particularly', 'with', 'the', 'scenes', 'concerning', 'Orton', 'and', 'Halliwell', 'and', 'the', 'sets', '(', 'particularly', 'of', 'their', 'flat', 'with', 'Halliwell', \"'s\", 'murals', 'decorating', 'every', 'surface', ')', 'are', 'terribly', 'well', 'done', '.']\n",
      "\n",
      "Review 29 Tokens:\n",
      "['I', 'thought', 'this', 'was', 'a', 'wonderful', 'way', 'to', 'spend', 'time', 'on', 'a', 'too', 'hot', 'summer', 'weekend', ',', 'sitting', 'in', 'the', 'air', 'conditioned', 'theater', 'and', 'watching', 'a', 'light-hearted', 'comedy', '.', 'The', 'plot', 'is', 'simplistic', ',', 'but', 'the', 'dialogue', 'is', 'witty', 'and', 'the', 'characters', 'are', 'likable', '(', 'even', 'the', 'well', 'bread', 'suspected', 'serial', 'killer', ')', '.', 'While', 'some', 'may', 'be', 'disappointed', 'when', 'they', 'realize', 'this', 'is', 'not', 'Match', 'Point', '2', ':', 'Risk', 'Addiction', ',', 'I', 'thought', 'it', 'was', 'proof', 'that', 'Woody', 'Allen', 'is', 'still', 'fully', 'in', 'control', 'of', 'the', 'style', 'many', 'of', 'us', 'have', 'grown', 'to', 'love.', 'br', '/', 'br', '/', 'This', 'was', 'the', 'most', 'I', \"'d\", 'laughed', 'at', 'one', 'of', 'Woody', \"'s\", 'comedies', 'in', 'years', '(', 'dare', 'I', 'say', 'a', 'decade', '?', ')', '.', 'While', 'I', \"'ve\", 'never', 'been', 'impressed', 'with', 'Scarlet', 'Johanson', ',', 'in', 'this', 'she', 'managed', 'to', 'tone', 'down', 'her', '``', 'sexy', \"''\", 'image', 'and', 'jumped', 'right', 'into', 'a', 'average', ',', 'but', 'spirited', 'young', 'woman.', 'br', '/', 'br', '/', 'This', 'may', 'not', 'be', 'the', 'crown', 'jewel', 'of', 'his', 'career', ',', 'but', 'it', 'was', 'wittier', 'than', '``', 'Devil', 'Wears', 'Prada', \"''\", 'and', 'more', 'interesting', 'than', '``', 'Superman', \"''\", 'a', 'great', 'comedy', 'to', 'go', 'see', 'with', 'friends', '.']\n",
      "\n",
      "Review 30 Tokens:\n",
      "['Basically', 'there', \"'s\", 'a', 'family', 'where', 'a', 'little', 'boy', '(', 'Jake', ')', 'thinks', 'there', \"'s\", 'a', 'zombie', 'in', 'his', 'closet', '&', 'his', 'parents', 'are', 'fighting', 'all', 'the', 'time.', 'br', '/', 'br', '/', 'This', 'movie', 'is', 'slower', 'than', 'a', 'soap', 'opera', '...', 'and', 'suddenly', ',', 'Jake', 'decides', 'to', 'become', 'Rambo', 'and', 'kill', 'the', 'zombie.', 'br', '/', 'br', '/', 'OK', ',', 'first', 'of', 'all', 'when', 'you', \"'re\", 'going', 'to', 'make', 'a', 'film', 'you', 'must', 'Decide', 'if', 'its', 'a', 'thriller', 'or', 'a', 'drama', '!', 'As', 'a', 'drama', 'the', 'movie', 'is', 'watchable', '.', 'Parents', 'are', 'divorcing', '&', 'arguing', 'like', 'in', 'real', 'life', '.', 'And', 'then', 'we', 'have', 'Jake', 'with', 'his', 'closet', 'which', 'totally', 'ruins', 'all', 'the', 'film', '!', 'I', 'expected', 'to', 'see', 'a', 'BOOGEYMAN', 'similar', 'movie', ',', 'and', 'instead', 'i', 'watched', 'a', 'drama', 'with', 'some', 'meaningless', 'thriller', 'spots.', 'br', '/', 'br', '/', '3', 'out', 'of', '10', 'just', 'for', 'the', 'well', 'playing', 'parents', '&', 'descent', 'dialogs', '.', 'As', 'for', 'the', 'shots', 'with', 'Jake', ':', 'just', 'ignore', 'them', '.']\n",
      "\n",
      "Review 31 Tokens:\n",
      "['Petter', 'Mattei', \"'s\", '``', 'Love', 'in', 'the', 'Time', 'of', 'Money', \"''\", 'is', 'a', 'visually', 'stunning', 'film', 'to', 'watch', '.', 'Mr.', 'Mattei', 'offers', 'us', 'a', 'vivid', 'portrait', 'about', 'human', 'relations', '.', 'This', 'is', 'a', 'movie', 'that', 'seems', 'to', 'be', 'telling', 'us', 'what', 'money', ',', 'power', 'and', 'success', 'do', 'to', 'people', 'in', 'the', 'different', 'situations', 'we', 'encounter', '.', 'br', '/', 'br', '/', 'This', 'being', 'a', 'variation', 'on', 'the', 'Arthur', 'Schnitzler', \"'s\", 'play', 'about', 'the', 'same', 'theme', ',', 'the', 'director', 'transfers', 'the', 'action', 'to', 'the', 'present', 'time', 'New', 'York', 'where', 'all', 'these', 'different', 'characters', 'meet', 'and', 'connect', '.', 'Each', 'one', 'is', 'connected', 'in', 'one', 'way', ',', 'or', 'another', 'to', 'the', 'next', 'person', ',', 'but', 'no', 'one', 'seems', 'to', 'know', 'the', 'previous', 'point', 'of', 'contact', '.', 'Stylishly', ',', 'the', 'film', 'has', 'a', 'sophisticated', 'luxurious', 'look', '.', 'We', 'are', 'taken', 'to', 'see', 'how', 'these', 'people', 'live', 'and', 'the', 'world', 'they', 'live', 'in', 'their', 'own', 'habitat.', 'br', '/', 'br', '/', 'The', 'only', 'thing', 'one', 'gets', 'out', 'of', 'all', 'these', 'souls', 'in', 'the', 'picture', 'is', 'the', 'different', 'stages', 'of', 'loneliness', 'each', 'one', 'inhabits', '.', 'A', 'big', 'city', 'is', 'not', 'exactly', 'the', 'best', 'place', 'in', 'which', 'human', 'relations', 'find', 'sincere', 'fulfillment', ',', 'as', 'one', 'discerns', 'is', 'the', 'case', 'with', 'most', 'of', 'the', 'people', 'we', 'encounter.', 'br', '/', 'br', '/', 'The', 'acting', 'is', 'good', 'under', 'Mr.', 'Mattei', \"'s\", 'direction', '.', 'Steve', 'Buscemi', ',', 'Rosario', 'Dawson', ',', 'Carol', 'Kane', ',', 'Michael', 'Imperioli', ',', 'Adrian', 'Grenier', ',', 'and', 'the', 'rest', 'of', 'the', 'talented', 'cast', ',', 'make', 'these', 'characters', 'come', 'alive.', 'br', '/', 'br', '/', 'We', 'wish', 'Mr.', 'Mattei', 'good', 'luck', 'and', 'await', 'anxiously', 'for', 'his', 'next', 'work', '.']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(f\"Review {i+27} Tokens:\")\n",
    "    print(imdb_data['tokens'][i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0277ced4-134c-4dda-935c-2e9efdbd9b76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated Tokens for Review 34:\n",
      "['One', 'of', 'the', 'other', 'reviewers', 'has', 'mentioned', 'that', 'after', 'watching', 'just', '1', 'Oz', 'episode', 'you', \"'ll\", 'be', 'hooked', '.', 'They', 'are', 'right', ',', 'as', 'this', 'is', 'exactly', 'what', 'happened', 'with', 'me.', 'The', 'first', 'thing', 'that', 'struck', 'me', 'about', 'Oz', 'was', 'its', 'and', 'unflinching', 'scenes', 'of', 'violence', ',', 'which', 'set', 'in', 'right', 'from', 'the', 'word', 'GO', '.', 'Trust', 'me', ',', 'this', 'is', 'not', 'a', 'show', 'for', 'the', 'faint', 'hearted', 'or', 'timid', '.', 'This', 'show', 'pulls', 'no', 'punches', 'with', 'regards', 'to', 'drugs', ',', 'sex', 'or', 'violence', '.', 'Its', 'is', 'hardcore', ',', 'in', 'the', 'classic', 'use', 'of', 'the', 'word.', 'It', 'is', 'called', 'OZ', 'as', 'that', 'is', 'the', 'nickname', 'given', 'to', 'the', 'Oswald', 'Maximum', 'Security', 'State', 'Penitentary', '.', 'It', 'focuses', 'mainly', 'on', 'Emerald', 'City', ',', 'an', 'experimental', 'section', 'of', 'the', 'prison', 'where', 'all', 'the', 'cells', 'have', 'glass', 'fronts', 'and', 'face', 'inwards', ',', 'so', 'privacy', 'is', 'not', 'high', 'on', 'the', 'agenda', '.', 'Em', 'City', 'is', 'home', 'to', 'many', '..', 'Aryans', ',', 'Muslims', ',', 'gangstas', ',', 'Latinos', ',', 'Christians', ',', 'Italians', ',', 'Irish', 'and', 'more', '....', 'so', 'scuffles', ',', 'death', 'stares', ',', 'dodgy', 'dealings', 'and', 'shady', 'agreements', 'are', 'never', 'far', 'away.', 'I', 'would', 'say', 'the', 'main', 'appeal', 'of', 'the', 'show', 'is', 'due', 'to', 'the', 'fact', 'that', 'it', 'goes', 'where', 'other', 'shows', 'would', \"n't\", 'dare', '.', 'Forget', 'pretty', 'pictures', 'painted', 'for', 'mainstream', 'audiences', ',', 'forget', 'charm', ',', 'forget', 'romance', '...', 'OZ', 'does', \"n't\", 'mess', 'around', '.', 'The', 'first', 'episode', 'I', 'ever', 'saw', 'struck', 'me', 'as', 'so', 'nasty', 'it', 'was', 'surreal', ',', 'I', 'could', \"n't\", 'say', 'I', 'was', 'ready', 'for', 'it', ',', 'but', 'as', 'I', 'watched', 'more', ',', 'I', 'developed', 'a', 'taste', 'for', 'Oz', ',', 'and', 'got', 'accustomed', 'to', 'the', 'high', 'levels', 'of', 'graphic', 'violence', '.', 'Not', 'just', 'violence', ',', 'but', 'injustice', '(', 'crooked', 'guards', 'who', \"'ll\", 'be', 'sold', 'out', 'for', 'a', 'nickel', ',', 'inmates', 'who', \"'ll\", 'kill', 'on', 'order', 'and', 'get', 'away', 'with', 'it', ',', 'well', 'mannered', ',', 'middle', 'class', 'inmates', 'being', 'turned', 'into', 'prison', 'bitches', 'due', 'to', 'their', 'lack', 'of', 'street', 'skills', 'or', 'prison', 'experience', ')', 'Watching', 'Oz', ',', 'you', 'may', 'become', 'comfortable', 'with', 'what', 'is', 'uncomfortable', 'viewing', '....', 'thats', 'if', 'you', 'can', 'get', 'in', 'touch', 'with', 'your', 'darker', 'side', '.']\n",
      "\n",
      "Updated Tokens for Review 35:\n",
      "['A', 'wonderful', 'little', 'production', '.', 'The', 'filming', 'technique', 'is', 'very', 'unassuming-', 'very', 'old-time-BBC', 'fashion', 'and', 'gives', 'a', 'comforting', ',', 'and', 'sometimes', 'discomforting', ',', 'sense', 'of', 'realism', 'to', 'the', 'entire', 'piece', '.', 'The', 'actors', 'are', 'extremely', 'well', 'chosen-', 'Michael', 'Sheen', 'not', 'only', '``', 'has', 'got', 'all', 'the', 'polari', \"''\", 'but', 'he', 'has', 'all', 'the', 'voices', 'down', 'pat', 'too', '!', 'You', 'can', 'truly', 'see', 'the', 'seamless', 'editing', 'guided', 'by', 'the', 'references', 'to', 'Williams', \"'\", 'diary', 'entries', ',', 'not', 'only', 'is', 'it', 'well', 'worth', 'the', 'watching', 'but', 'it', 'is', 'a', 'terrificly', 'written', 'and', 'performed', 'piece', '.', 'A', 'masterful', 'production', 'about', 'one', 'of', 'the', 'great', 'master', \"'s\", 'of', 'comedy', 'and', 'his', 'life', '.', 'The', 'realism', 'really', 'comes', 'home', 'with', 'the', 'little', 'things', ':', 'the', 'fantasy', 'of', 'the', 'guard', 'which', ',', 'rather', 'than', 'use', 'the', 'traditional', \"'dream\", \"'\", 'techniques', 'remains', 'solid', 'then', 'disappears', '.', 'It', 'plays', 'on', 'our', 'knowledge', 'and', 'our', 'senses', ',', 'particularly', 'with', 'the', 'scenes', 'concerning', 'Orton', 'and', 'Halliwell', 'and', 'the', 'sets', '(', 'particularly', 'of', 'their', 'flat', 'with', 'Halliwell', \"'s\", 'murals', 'decorating', 'every', 'surface', ')', 'are', 'terribly', 'well', 'done', '.']\n",
      "\n",
      "Updated Tokens for Review 36:\n",
      "['I', 'thought', 'this', 'was', 'a', 'wonderful', 'way', 'to', 'spend', 'time', 'on', 'a', 'too', 'hot', 'summer', 'weekend', ',', 'sitting', 'in', 'the', 'air', 'conditioned', 'theater', 'and', 'watching', 'a', 'light-hearted', 'comedy', '.', 'The', 'plot', 'is', 'simplistic', ',', 'but', 'the', 'dialogue', 'is', 'witty', 'and', 'the', 'characters', 'are', 'likable', '(', 'even', 'the', 'well', 'suspected', 'serial', 'killer', ')', '.', 'While', 'some', 'may', 'be', 'disappointed', 'when', 'they', 'realize', 'this', 'is', 'not', 'Match', 'Point', '2', ':', 'Risk', 'Addiction', ',', 'I', 'thought', 'it', 'was', 'proof', 'that', 'Woody', 'Allen', 'is', 'still', 'fully', 'in', 'control', 'of', 'the', 'style', 'many', 'of', 'us', 'have', 'grown', 'to', 'love.', 'This', 'was', 'the', 'most', 'I', \"'d\", 'laughed', 'at', 'one', 'of', 'Woody', \"'s\", 'comedies', 'in', 'years', '(', 'dare', 'I', 'say', 'a', 'decade', '?', ')', '.', 'While', 'I', \"'ve\", 'never', 'been', 'impressed', 'with', 'Scarlet', 'Johanson', ',', 'in', 'this', 'she', 'managed', 'to', 'tone', 'down', 'her', '``', 'sexy', \"''\", 'image', 'and', 'jumped', 'right', 'into', 'a', 'average', ',', 'but', 'spirited', 'young', 'woman.', 'This', 'may', 'not', 'be', 'the', 'crown', 'jewel', 'of', 'his', 'career', ',', 'but', 'it', 'was', 'wittier', 'than', '``', 'Devil', 'Wears', 'Prada', \"''\", 'and', 'more', 'interesting', 'than', '``', 'Superman', \"''\", 'a', 'great', 'comedy', 'to', 'go', 'see', 'with', 'friends', '.']\n",
      "\n",
      "Updated Tokens for Review 37:\n",
      "['Basically', 'there', \"'s\", 'a', 'family', 'where', 'a', 'little', 'boy', '(', 'Jake', ')', 'thinks', 'there', \"'s\", 'a', 'zombie', 'in', 'his', 'closet', '&', 'his', 'parents', 'are', 'fighting', 'all', 'the', 'time.', 'This', 'movie', 'is', 'slower', 'than', 'a', 'soap', 'opera', '...', 'and', 'suddenly', ',', 'Jake', 'decides', 'to', 'become', 'Rambo', 'and', 'kill', 'the', 'zombie.', 'OK', ',', 'first', 'of', 'all', 'when', 'you', \"'re\", 'going', 'to', 'make', 'a', 'film', 'you', 'must', 'Decide', 'if', 'its', 'a', 'thriller', 'or', 'a', 'drama', '!', 'As', 'a', 'drama', 'the', 'movie', 'is', 'watchable', '.', 'Parents', 'are', 'divorcing', '&', 'arguing', 'like', 'in', 'real', 'life', '.', 'And', 'then', 'we', 'have', 'Jake', 'with', 'his', 'closet', 'which', 'totally', 'ruins', 'all', 'the', 'film', '!', 'I', 'expected', 'to', 'see', 'a', 'BOOGEYMAN', 'similar', 'movie', ',', 'and', 'instead', 'i', 'watched', 'a', 'drama', 'with', 'some', 'meaningless', 'thriller', 'spots.', '3', 'out', 'of', '10', 'just', 'for', 'the', 'well', 'playing', 'parents', '&', 'descent', 'dialogs', '.', 'As', 'for', 'the', 'shots', 'with', 'Jake', ':', 'just', 'ignore', 'them', '.']\n",
      "\n",
      "Updated Tokens for Review 38:\n",
      "['Petter', 'Mattei', \"'s\", '``', 'Love', 'in', 'the', 'Time', 'of', 'Money', \"''\", 'is', 'a', 'visually', 'stunning', 'film', 'to', 'watch', '.', 'Mr.', 'Mattei', 'offers', 'us', 'a', 'vivid', 'portrait', 'about', 'human', 'relations', '.', 'This', 'is', 'a', 'movie', 'that', 'seems', 'to', 'be', 'telling', 'us', 'what', 'money', ',', 'power', 'and', 'success', 'do', 'to', 'people', 'in', 'the', 'different', 'situations', 'we', 'encounter', '.', 'This', 'being', 'a', 'variation', 'on', 'the', 'Arthur', 'Schnitzler', \"'s\", 'play', 'about', 'the', 'same', 'theme', ',', 'the', 'director', 'transfers', 'the', 'action', 'to', 'the', 'present', 'time', 'New', 'York', 'where', 'all', 'these', 'different', 'characters', 'meet', 'and', 'connect', '.', 'Each', 'one', 'is', 'connected', 'in', 'one', 'way', ',', 'or', 'another', 'to', 'the', 'next', 'person', ',', 'but', 'no', 'one', 'seems', 'to', 'know', 'the', 'previous', 'point', 'of', 'contact', '.', 'Stylishly', ',', 'the', 'film', 'has', 'a', 'sophisticated', 'luxurious', 'look', '.', 'We', 'are', 'taken', 'to', 'see', 'how', 'these', 'people', 'live', 'and', 'the', 'world', 'they', 'live', 'in', 'their', 'own', 'habitat.', 'The', 'only', 'thing', 'one', 'gets', 'out', 'of', 'all', 'these', 'souls', 'in', 'the', 'picture', 'is', 'the', 'different', 'stages', 'of', 'loneliness', 'each', 'one', 'inhabits', '.', 'A', 'big', 'city', 'is', 'not', 'exactly', 'the', 'best', 'place', 'in', 'which', 'human', 'relations', 'find', 'sincere', 'fulfillment', ',', 'as', 'one', 'discerns', 'is', 'the', 'case', 'with', 'most', 'of', 'the', 'people', 'we', 'encounter.', 'The', 'acting', 'is', 'good', 'under', 'Mr.', 'Mattei', \"'s\", 'direction', '.', 'Steve', 'Buscemi', ',', 'Rosario', 'Dawson', ',', 'Carol', 'Kane', ',', 'Michael', 'Imperioli', ',', 'Adrian', 'Grenier', ',', 'and', 'the', 'rest', 'of', 'the', 'talented', 'cast', ',', 'make', 'these', 'characters', 'come', 'alive.', 'We', 'wish', 'Mr.', 'Mattei', 'good', 'luck', 'and', 'await', 'anxiously', 'for', 'his', 'next', 'work', '.']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Function to remove tokens containing 'br' or '/'\n",
    "def remove_br_tokens(tokens):\n",
    "    return [token for token in tokens if 'br' not in token and '/' not in token]\n",
    "\n",
    "# Apply the function to your 'tokens' column\n",
    "imdb_data['tokens'] = imdb_data['tokens'].apply(remove_br_tokens)\n",
    "\n",
    "# Print the updated tokens for the first few rows\n",
    "for i in range(5):\n",
    "    print(f\"Updated Tokens for Review {i+34}:\")\n",
    "    print(imdb_data['tokens'][i])\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "60d762e9-b128-4d0a-bb31-44cae9ba405f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated Tokens for Review 59:\n",
      "['one', 'of', 'the', 'other', 'reviewers', 'has', 'mentioned', 'that', 'after', 'watching', 'just', '1', 'oz', 'episode', 'you', \"'ll\", 'be', 'hooked', '.', 'they', 'are', 'right', ',', 'as', 'this', 'is', 'exactly', 'what', 'happened', 'with', 'me.', 'the', 'first', 'thing', 'that', 'struck', 'me', 'about', 'oz', 'was', 'its', 'and', 'unflinching', 'scenes', 'of', 'violence', ',', 'which', 'set', 'in', 'right', 'from', 'the', 'word', 'go', '.', 'trust', 'me', ',', 'this', 'is', 'not', 'a', 'show', 'for', 'the', 'faint', 'hearted', 'or', 'timid', '.', 'this', 'show', 'pulls', 'no', 'punches', 'with', 'regards', 'to', 'drugs', ',', 'sex', 'or', 'violence', '.', 'its', 'is', 'hardcore', ',', 'in', 'the', 'classic', 'use', 'of', 'the', 'word.', 'it', 'is', 'called', 'oz', 'as', 'that', 'is', 'the', 'nickname', 'given', 'to', 'the', 'oswald', 'maximum', 'security', 'state', 'penitentary', '.', 'it', 'focuses', 'mainly', 'on', 'emerald', 'city', ',', 'an', 'experimental', 'section', 'of', 'the', 'prison', 'where', 'all', 'the', 'cells', 'have', 'glass', 'fronts', 'and', 'face', 'inwards', ',', 'so', 'privacy', 'is', 'not', 'high', 'on', 'the', 'agenda', '.', 'em', 'city', 'is', 'home', 'to', 'many', '..', 'aryans', ',', 'muslims', ',', 'gangstas', ',', 'latinos', ',', 'christians', ',', 'italians', ',', 'irish', 'and', 'more', '....', 'so', 'scuffles', ',', 'death', 'stares', ',', 'dodgy', 'dealings', 'and', 'shady', 'agreements', 'are', 'never', 'far', 'away.', 'i', 'would', 'say', 'the', 'main', 'appeal', 'of', 'the', 'show', 'is', 'due', 'to', 'the', 'fact', 'that', 'it', 'goes', 'where', 'other', 'shows', 'would', \"n't\", 'dare', '.', 'forget', 'pretty', 'pictures', 'painted', 'for', 'mainstream', 'audiences', ',', 'forget', 'charm', ',', 'forget', 'romance', '...', 'oz', 'does', \"n't\", 'mess', 'around', '.', 'the', 'first', 'episode', 'i', 'ever', 'saw', 'struck', 'me', 'as', 'so', 'nasty', 'it', 'was', 'surreal', ',', 'i', 'could', \"n't\", 'say', 'i', 'was', 'ready', 'for', 'it', ',', 'but', 'as', 'i', 'watched', 'more', ',', 'i', 'developed', 'a', 'taste', 'for', 'oz', ',', 'and', 'got', 'accustomed', 'to', 'the', 'high', 'levels', 'of', 'graphic', 'violence', '.', 'not', 'just', 'violence', ',', 'but', 'injustice', '(', 'crooked', 'guards', 'who', \"'ll\", 'be', 'sold', 'out', 'for', 'a', 'nickel', ',', 'inmates', 'who', \"'ll\", 'kill', 'on', 'order', 'and', 'get', 'away', 'with', 'it', ',', 'well', 'mannered', ',', 'middle', 'class', 'inmates', 'being', 'turned', 'into', 'prison', 'bitches', 'due', 'to', 'their', 'lack', 'of', 'street', 'skills', 'or', 'prison', 'experience', ')', 'watching', 'oz', ',', 'you', 'may', 'become', 'comfortable', 'with', 'what', 'is', 'uncomfortable', 'viewing', '....', 'thats', 'if', 'you', 'can', 'get', 'in', 'touch', 'with', 'your', 'darker', 'side', '.']\n",
      "\n",
      "Updated Tokens for Review 60:\n",
      "['a', 'wonderful', 'little', 'production', '.', 'the', 'filming', 'technique', 'is', 'very', 'unassuming-', 'very', 'old-time-bbc', 'fashion', 'and', 'gives', 'a', 'comforting', ',', 'and', 'sometimes', 'discomforting', ',', 'sense', 'of', 'realism', 'to', 'the', 'entire', 'piece', '.', 'the', 'actors', 'are', 'extremely', 'well', 'chosen-', 'michael', 'sheen', 'not', 'only', '``', 'has', 'got', 'all', 'the', 'polari', \"''\", 'but', 'he', 'has', 'all', 'the', 'voices', 'down', 'pat', 'too', '!', 'you', 'can', 'truly', 'see', 'the', 'seamless', 'editing', 'guided', 'by', 'the', 'references', 'to', 'williams', \"'\", 'diary', 'entries', ',', 'not', 'only', 'is', 'it', 'well', 'worth', 'the', 'watching', 'but', 'it', 'is', 'a', 'terrificly', 'written', 'and', 'performed', 'piece', '.', 'a', 'masterful', 'production', 'about', 'one', 'of', 'the', 'great', 'master', \"'s\", 'of', 'comedy', 'and', 'his', 'life', '.', 'the', 'realism', 'really', 'comes', 'home', 'with', 'the', 'little', 'things', ':', 'the', 'fantasy', 'of', 'the', 'guard', 'which', ',', 'rather', 'than', 'use', 'the', 'traditional', \"'dream\", \"'\", 'techniques', 'remains', 'solid', 'then', 'disappears', '.', 'it', 'plays', 'on', 'our', 'knowledge', 'and', 'our', 'senses', ',', 'particularly', 'with', 'the', 'scenes', 'concerning', 'orton', 'and', 'halliwell', 'and', 'the', 'sets', '(', 'particularly', 'of', 'their', 'flat', 'with', 'halliwell', \"'s\", 'murals', 'decorating', 'every', 'surface', ')', 'are', 'terribly', 'well', 'done', '.']\n",
      "\n",
      "Updated Tokens for Review 61:\n",
      "['i', 'thought', 'this', 'was', 'a', 'wonderful', 'way', 'to', 'spend', 'time', 'on', 'a', 'too', 'hot', 'summer', 'weekend', ',', 'sitting', 'in', 'the', 'air', 'conditioned', 'theater', 'and', 'watching', 'a', 'light-hearted', 'comedy', '.', 'the', 'plot', 'is', 'simplistic', ',', 'but', 'the', 'dialogue', 'is', 'witty', 'and', 'the', 'characters', 'are', 'likable', '(', 'even', 'the', 'well', 'suspected', 'serial', 'killer', ')', '.', 'while', 'some', 'may', 'be', 'disappointed', 'when', 'they', 'realize', 'this', 'is', 'not', 'match', 'point', '2', ':', 'risk', 'addiction', ',', 'i', 'thought', 'it', 'was', 'proof', 'that', 'woody', 'allen', 'is', 'still', 'fully', 'in', 'control', 'of', 'the', 'style', 'many', 'of', 'us', 'have', 'grown', 'to', 'love.', 'this', 'was', 'the', 'most', 'i', \"'d\", 'laughed', 'at', 'one', 'of', 'woody', \"'s\", 'comedies', 'in', 'years', '(', 'dare', 'i', 'say', 'a', 'decade', '?', ')', '.', 'while', 'i', \"'ve\", 'never', 'been', 'impressed', 'with', 'scarlet', 'johanson', ',', 'in', 'this', 'she', 'managed', 'to', 'tone', 'down', 'her', '``', 'sexy', \"''\", 'image', 'and', 'jumped', 'right', 'into', 'a', 'average', ',', 'but', 'spirited', 'young', 'woman.', 'this', 'may', 'not', 'be', 'the', 'crown', 'jewel', 'of', 'his', 'career', ',', 'but', 'it', 'was', 'wittier', 'than', '``', 'devil', 'wears', 'prada', \"''\", 'and', 'more', 'interesting', 'than', '``', 'superman', \"''\", 'a', 'great', 'comedy', 'to', 'go', 'see', 'with', 'friends', '.']\n",
      "\n",
      "Updated Tokens for Review 62:\n",
      "['basically', 'there', \"'s\", 'a', 'family', 'where', 'a', 'little', 'boy', '(', 'jake', ')', 'thinks', 'there', \"'s\", 'a', 'zombie', 'in', 'his', 'closet', '&', 'his', 'parents', 'are', 'fighting', 'all', 'the', 'time.', 'this', 'movie', 'is', 'slower', 'than', 'a', 'soap', 'opera', '...', 'and', 'suddenly', ',', 'jake', 'decides', 'to', 'become', 'rambo', 'and', 'kill', 'the', 'zombie.', 'ok', ',', 'first', 'of', 'all', 'when', 'you', \"'re\", 'going', 'to', 'make', 'a', 'film', 'you', 'must', 'decide', 'if', 'its', 'a', 'thriller', 'or', 'a', 'drama', '!', 'as', 'a', 'drama', 'the', 'movie', 'is', 'watchable', '.', 'parents', 'are', 'divorcing', '&', 'arguing', 'like', 'in', 'real', 'life', '.', 'and', 'then', 'we', 'have', 'jake', 'with', 'his', 'closet', 'which', 'totally', 'ruins', 'all', 'the', 'film', '!', 'i', 'expected', 'to', 'see', 'a', 'boogeyman', 'similar', 'movie', ',', 'and', 'instead', 'i', 'watched', 'a', 'drama', 'with', 'some', 'meaningless', 'thriller', 'spots.', '3', 'out', 'of', '10', 'just', 'for', 'the', 'well', 'playing', 'parents', '&', 'descent', 'dialogs', '.', 'as', 'for', 'the', 'shots', 'with', 'jake', ':', 'just', 'ignore', 'them', '.']\n",
      "\n",
      "Updated Tokens for Review 63:\n",
      "['petter', 'mattei', \"'s\", '``', 'love', 'in', 'the', 'time', 'of', 'money', \"''\", 'is', 'a', 'visually', 'stunning', 'film', 'to', 'watch', '.', 'mr.', 'mattei', 'offers', 'us', 'a', 'vivid', 'portrait', 'about', 'human', 'relations', '.', 'this', 'is', 'a', 'movie', 'that', 'seems', 'to', 'be', 'telling', 'us', 'what', 'money', ',', 'power', 'and', 'success', 'do', 'to', 'people', 'in', 'the', 'different', 'situations', 'we', 'encounter', '.', 'this', 'being', 'a', 'variation', 'on', 'the', 'arthur', 'schnitzler', \"'s\", 'play', 'about', 'the', 'same', 'theme', ',', 'the', 'director', 'transfers', 'the', 'action', 'to', 'the', 'present', 'time', 'new', 'york', 'where', 'all', 'these', 'different', 'characters', 'meet', 'and', 'connect', '.', 'each', 'one', 'is', 'connected', 'in', 'one', 'way', ',', 'or', 'another', 'to', 'the', 'next', 'person', ',', 'but', 'no', 'one', 'seems', 'to', 'know', 'the', 'previous', 'point', 'of', 'contact', '.', 'stylishly', ',', 'the', 'film', 'has', 'a', 'sophisticated', 'luxurious', 'look', '.', 'we', 'are', 'taken', 'to', 'see', 'how', 'these', 'people', 'live', 'and', 'the', 'world', 'they', 'live', 'in', 'their', 'own', 'habitat.', 'the', 'only', 'thing', 'one', 'gets', 'out', 'of', 'all', 'these', 'souls', 'in', 'the', 'picture', 'is', 'the', 'different', 'stages', 'of', 'loneliness', 'each', 'one', 'inhabits', '.', 'a', 'big', 'city', 'is', 'not', 'exactly', 'the', 'best', 'place', 'in', 'which', 'human', 'relations', 'find', 'sincere', 'fulfillment', ',', 'as', 'one', 'discerns', 'is', 'the', 'case', 'with', 'most', 'of', 'the', 'people', 'we', 'encounter.', 'the', 'acting', 'is', 'good', 'under', 'mr.', 'mattei', \"'s\", 'direction', '.', 'steve', 'buscemi', ',', 'rosario', 'dawson', ',', 'carol', 'kane', ',', 'michael', 'imperioli', ',', 'adrian', 'grenier', ',', 'and', 'the', 'rest', 'of', 'the', 'talented', 'cast', ',', 'make', 'these', 'characters', 'come', 'alive.', 'we', 'wish', 'mr.', 'mattei', 'good', 'luck', 'and', 'await', 'anxiously', 'for', 'his', 'next', 'work', '.']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Function to convert tokens to lowercase\n",
    "def lowercase_tokens(tokens):\n",
    "    return [token.lower() for token in tokens]\n",
    "\n",
    "# Apply the function to your 'tokens' column\n",
    "imdb_data['tokens'] = imdb_data['tokens'].apply(lowercase_tokens)\n",
    "\n",
    "# Print the updated tokens for the first few rows\n",
    "for i in range(5):\n",
    "    print(f\"Updated Tokens for Review {i+59}:\")\n",
    "    print(imdb_data['tokens'][i])\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4d2691b0-b214-424e-86cf-07e686c1ddca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated Tokens for Review 64:\n",
      "['one', 'of', 'the', 'other', 'reviewers', 'has', 'mentioned', 'that', 'after', 'watching', 'just', '1', 'oz', 'episode', 'you', \"'ll\", 'be', 'hooked', 'they', 'are', 'right', 'as', 'this', 'is', 'exactly', 'what', 'happened', 'with', 'me.', 'the', 'first', 'thing', 'that', 'struck', 'me', 'about', 'oz', 'was', 'its', 'and', 'unflinching', 'scenes', 'of', 'violence', 'which', 'set', 'in', 'right', 'from', 'the', 'word', 'go', 'trust', 'me', 'this', 'is', 'not', 'a', 'show', 'for', 'the', 'faint', 'hearted', 'or', 'timid', 'this', 'show', 'pulls', 'no', 'punches', 'with', 'regards', 'to', 'drugs', 'sex', 'or', 'violence', 'its', 'is', 'hardcore', 'in', 'the', 'classic', 'use', 'of', 'the', 'word.', 'it', 'is', 'called', 'oz', 'as', 'that', 'is', 'the', 'nickname', 'given', 'to', 'the', 'oswald', 'maximum', 'security', 'state', 'penitentary', 'it', 'focuses', 'mainly', 'on', 'emerald', 'city', 'an', 'experimental', 'section', 'of', 'the', 'prison', 'where', 'all', 'the', 'cells', 'have', 'glass', 'fronts', 'and', 'face', 'inwards', 'so', 'privacy', 'is', 'not', 'high', 'on', 'the', 'agenda', 'em', 'city', 'is', 'home', 'to', 'many', '..', 'aryans', 'muslims', 'gangstas', 'latinos', 'christians', 'italians', 'irish', 'and', 'more', '....', 'so', 'scuffles', 'death', 'stares', 'dodgy', 'dealings', 'and', 'shady', 'agreements', 'are', 'never', 'far', 'away.', 'i', 'would', 'say', 'the', 'main', 'appeal', 'of', 'the', 'show', 'is', 'due', 'to', 'the', 'fact', 'that', 'it', 'goes', 'where', 'other', 'shows', 'would', \"n't\", 'dare', 'forget', 'pretty', 'pictures', 'painted', 'for', 'mainstream', 'audiences', 'forget', 'charm', 'forget', 'romance', '...', 'oz', 'does', \"n't\", 'mess', 'around', 'the', 'first', 'episode', 'i', 'ever', 'saw', 'struck', 'me', 'as', 'so', 'nasty', 'it', 'was', 'surreal', 'i', 'could', \"n't\", 'say', 'i', 'was', 'ready', 'for', 'it', 'but', 'as', 'i', 'watched', 'more', 'i', 'developed', 'a', 'taste', 'for', 'oz', 'and', 'got', 'accustomed', 'to', 'the', 'high', 'levels', 'of', 'graphic', 'violence', 'not', 'just', 'violence', 'but', 'injustice', 'crooked', 'guards', 'who', \"'ll\", 'be', 'sold', 'out', 'for', 'a', 'nickel', 'inmates', 'who', \"'ll\", 'kill', 'on', 'order', 'and', 'get', 'away', 'with', 'it', 'well', 'mannered', 'middle', 'class', 'inmates', 'being', 'turned', 'into', 'prison', 'bitches', 'due', 'to', 'their', 'lack', 'of', 'street', 'skills', 'or', 'prison', 'experience', 'watching', 'oz', 'you', 'may', 'become', 'comfortable', 'with', 'what', 'is', 'uncomfortable', 'viewing', '....', 'thats', 'if', 'you', 'can', 'get', 'in', 'touch', 'with', 'your', 'darker', 'side']\n",
      "\n",
      "Updated Tokens for Review 65:\n",
      "['a', 'wonderful', 'little', 'production', 'the', 'filming', 'technique', 'is', 'very', 'unassuming-', 'very', 'old-time-bbc', 'fashion', 'and', 'gives', 'a', 'comforting', 'and', 'sometimes', 'discomforting', 'sense', 'of', 'realism', 'to', 'the', 'entire', 'piece', 'the', 'actors', 'are', 'extremely', 'well', 'chosen-', 'michael', 'sheen', 'not', 'only', '``', 'has', 'got', 'all', 'the', 'polari', \"''\", 'but', 'he', 'has', 'all', 'the', 'voices', 'down', 'pat', 'too', 'you', 'can', 'truly', 'see', 'the', 'seamless', 'editing', 'guided', 'by', 'the', 'references', 'to', 'williams', 'diary', 'entries', 'not', 'only', 'is', 'it', 'well', 'worth', 'the', 'watching', 'but', 'it', 'is', 'a', 'terrificly', 'written', 'and', 'performed', 'piece', 'a', 'masterful', 'production', 'about', 'one', 'of', 'the', 'great', 'master', \"'s\", 'of', 'comedy', 'and', 'his', 'life', 'the', 'realism', 'really', 'comes', 'home', 'with', 'the', 'little', 'things', 'the', 'fantasy', 'of', 'the', 'guard', 'which', 'rather', 'than', 'use', 'the', 'traditional', \"'dream\", 'techniques', 'remains', 'solid', 'then', 'disappears', 'it', 'plays', 'on', 'our', 'knowledge', 'and', 'our', 'senses', 'particularly', 'with', 'the', 'scenes', 'concerning', 'orton', 'and', 'halliwell', 'and', 'the', 'sets', 'particularly', 'of', 'their', 'flat', 'with', 'halliwell', \"'s\", 'murals', 'decorating', 'every', 'surface', 'are', 'terribly', 'well', 'done']\n",
      "\n",
      "Updated Tokens for Review 66:\n",
      "['i', 'thought', 'this', 'was', 'a', 'wonderful', 'way', 'to', 'spend', 'time', 'on', 'a', 'too', 'hot', 'summer', 'weekend', 'sitting', 'in', 'the', 'air', 'conditioned', 'theater', 'and', 'watching', 'a', 'light-hearted', 'comedy', 'the', 'plot', 'is', 'simplistic', 'but', 'the', 'dialogue', 'is', 'witty', 'and', 'the', 'characters', 'are', 'likable', 'even', 'the', 'well', 'suspected', 'serial', 'killer', 'while', 'some', 'may', 'be', 'disappointed', 'when', 'they', 'realize', 'this', 'is', 'not', 'match', 'point', '2', 'risk', 'addiction', 'i', 'thought', 'it', 'was', 'proof', 'that', 'woody', 'allen', 'is', 'still', 'fully', 'in', 'control', 'of', 'the', 'style', 'many', 'of', 'us', 'have', 'grown', 'to', 'love.', 'this', 'was', 'the', 'most', 'i', \"'d\", 'laughed', 'at', 'one', 'of', 'woody', \"'s\", 'comedies', 'in', 'years', 'dare', 'i', 'say', 'a', 'decade', 'while', 'i', \"'ve\", 'never', 'been', 'impressed', 'with', 'scarlet', 'johanson', 'in', 'this', 'she', 'managed', 'to', 'tone', 'down', 'her', '``', 'sexy', \"''\", 'image', 'and', 'jumped', 'right', 'into', 'a', 'average', 'but', 'spirited', 'young', 'woman.', 'this', 'may', 'not', 'be', 'the', 'crown', 'jewel', 'of', 'his', 'career', 'but', 'it', 'was', 'wittier', 'than', '``', 'devil', 'wears', 'prada', \"''\", 'and', 'more', 'interesting', 'than', '``', 'superman', \"''\", 'a', 'great', 'comedy', 'to', 'go', 'see', 'with', 'friends']\n",
      "\n",
      "Updated Tokens for Review 67:\n",
      "['basically', 'there', \"'s\", 'a', 'family', 'where', 'a', 'little', 'boy', 'jake', 'thinks', 'there', \"'s\", 'a', 'zombie', 'in', 'his', 'closet', 'his', 'parents', 'are', 'fighting', 'all', 'the', 'time.', 'this', 'movie', 'is', 'slower', 'than', 'a', 'soap', 'opera', '...', 'and', 'suddenly', 'jake', 'decides', 'to', 'become', 'rambo', 'and', 'kill', 'the', 'zombie.', 'ok', 'first', 'of', 'all', 'when', 'you', \"'re\", 'going', 'to', 'make', 'a', 'film', 'you', 'must', 'decide', 'if', 'its', 'a', 'thriller', 'or', 'a', 'drama', 'as', 'a', 'drama', 'the', 'movie', 'is', 'watchable', 'parents', 'are', 'divorcing', 'arguing', 'like', 'in', 'real', 'life', 'and', 'then', 'we', 'have', 'jake', 'with', 'his', 'closet', 'which', 'totally', 'ruins', 'all', 'the', 'film', 'i', 'expected', 'to', 'see', 'a', 'boogeyman', 'similar', 'movie', 'and', 'instead', 'i', 'watched', 'a', 'drama', 'with', 'some', 'meaningless', 'thriller', 'spots.', '3', 'out', 'of', '10', 'just', 'for', 'the', 'well', 'playing', 'parents', 'descent', 'dialogs', 'as', 'for', 'the', 'shots', 'with', 'jake', 'just', 'ignore', 'them']\n",
      "\n",
      "Updated Tokens for Review 68:\n",
      "['petter', 'mattei', \"'s\", '``', 'love', 'in', 'the', 'time', 'of', 'money', \"''\", 'is', 'a', 'visually', 'stunning', 'film', 'to', 'watch', 'mr.', 'mattei', 'offers', 'us', 'a', 'vivid', 'portrait', 'about', 'human', 'relations', 'this', 'is', 'a', 'movie', 'that', 'seems', 'to', 'be', 'telling', 'us', 'what', 'money', 'power', 'and', 'success', 'do', 'to', 'people', 'in', 'the', 'different', 'situations', 'we', 'encounter', 'this', 'being', 'a', 'variation', 'on', 'the', 'arthur', 'schnitzler', \"'s\", 'play', 'about', 'the', 'same', 'theme', 'the', 'director', 'transfers', 'the', 'action', 'to', 'the', 'present', 'time', 'new', 'york', 'where', 'all', 'these', 'different', 'characters', 'meet', 'and', 'connect', 'each', 'one', 'is', 'connected', 'in', 'one', 'way', 'or', 'another', 'to', 'the', 'next', 'person', 'but', 'no', 'one', 'seems', 'to', 'know', 'the', 'previous', 'point', 'of', 'contact', 'stylishly', 'the', 'film', 'has', 'a', 'sophisticated', 'luxurious', 'look', 'we', 'are', 'taken', 'to', 'see', 'how', 'these', 'people', 'live', 'and', 'the', 'world', 'they', 'live', 'in', 'their', 'own', 'habitat.', 'the', 'only', 'thing', 'one', 'gets', 'out', 'of', 'all', 'these', 'souls', 'in', 'the', 'picture', 'is', 'the', 'different', 'stages', 'of', 'loneliness', 'each', 'one', 'inhabits', 'a', 'big', 'city', 'is', 'not', 'exactly', 'the', 'best', 'place', 'in', 'which', 'human', 'relations', 'find', 'sincere', 'fulfillment', 'as', 'one', 'discerns', 'is', 'the', 'case', 'with', 'most', 'of', 'the', 'people', 'we', 'encounter.', 'the', 'acting', 'is', 'good', 'under', 'mr.', 'mattei', \"'s\", 'direction', 'steve', 'buscemi', 'rosario', 'dawson', 'carol', 'kane', 'michael', 'imperioli', 'adrian', 'grenier', 'and', 'the', 'rest', 'of', 'the', 'talented', 'cast', 'make', 'these', 'characters', 'come', 'alive.', 'we', 'wish', 'mr.', 'mattei', 'good', 'luck', 'and', 'await', 'anxiously', 'for', 'his', 'next', 'work']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def remove_punctuation(tokens):\n",
    "    return [token for token in tokens if token not in string.punctuation]\n",
    "\n",
    "# Apply the function to your 'tokens' column\n",
    "imdb_data['tokens'] = imdb_data['tokens'].apply(remove_punctuation)\n",
    "\n",
    "# Print the updated tokens for the first few rows\n",
    "for i in range(5):\n",
    "    print(f\"Updated Tokens for Review {i+64}:\")\n",
    "    print(imdb_data['tokens'][i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a4d3dcff-9f14-48a9-920c-2831c7dba702",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\MITUL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated Tokens for Review 1:\n",
      "['one', 'reviewers', 'mentioned', 'watching', '1', 'oz', 'episode', \"'ll\", 'hooked', 'right', 'exactly', 'happened', 'me.', 'first', 'thing', 'struck', 'oz', 'unflinching', 'scenes', 'violence', 'set', 'right', 'word', 'go', 'trust', 'show', 'faint', 'hearted', 'timid', 'show', 'pulls', 'punches', 'regards', 'drugs', 'sex', 'violence', 'hardcore', 'classic', 'use', 'word.', 'called', 'oz', 'nickname', 'given', 'oswald', 'maximum', 'security', 'state', 'penitentary', 'focuses', 'mainly', 'emerald', 'city', 'experimental', 'section', 'prison', 'cells', 'glass', 'fronts', 'face', 'inwards', 'privacy', 'high', 'agenda', 'em', 'city', 'home', 'many', '..', 'aryans', 'muslims', 'gangstas', 'latinos', 'christians', 'italians', 'irish', '....', 'scuffles', 'death', 'stares', 'dodgy', 'dealings', 'shady', 'agreements', 'never', 'far', 'away.', 'would', 'say', 'main', 'appeal', 'show', 'due', 'fact', 'goes', 'shows', 'would', \"n't\", 'dare', 'forget', 'pretty', 'pictures', 'painted', 'mainstream', 'audiences', 'forget', 'charm', 'forget', 'romance', '...', 'oz', \"n't\", 'mess', 'around', 'first', 'episode', 'ever', 'saw', 'struck', 'nasty', 'surreal', 'could', \"n't\", 'say', 'ready', 'watched', 'developed', 'taste', 'oz', 'got', 'accustomed', 'high', 'levels', 'graphic', 'violence', 'violence', 'injustice', 'crooked', 'guards', \"'ll\", 'sold', 'nickel', 'inmates', \"'ll\", 'kill', 'order', 'get', 'away', 'well', 'mannered', 'middle', 'class', 'inmates', 'turned', 'prison', 'bitches', 'due', 'lack', 'street', 'skills', 'prison', 'experience', 'watching', 'oz', 'may', 'become', 'comfortable', 'uncomfortable', 'viewing', '....', 'thats', 'get', 'touch', 'darker', 'side']\n",
      "\n",
      "Updated Tokens for Review 2:\n",
      "['wonderful', 'little', 'production', 'filming', 'technique', 'unassuming-', 'old-time-bbc', 'fashion', 'gives', 'comforting', 'sometimes', 'discomforting', 'sense', 'realism', 'entire', 'piece', 'actors', 'extremely', 'well', 'chosen-', 'michael', 'sheen', '``', 'got', 'polari', \"''\", 'voices', 'pat', 'truly', 'see', 'seamless', 'editing', 'guided', 'references', 'williams', 'diary', 'entries', 'well', 'worth', 'watching', 'terrificly', 'written', 'performed', 'piece', 'masterful', 'production', 'one', 'great', 'master', \"'s\", 'comedy', 'life', 'realism', 'really', 'comes', 'home', 'little', 'things', 'fantasy', 'guard', 'rather', 'use', 'traditional', \"'dream\", 'techniques', 'remains', 'solid', 'disappears', 'plays', 'knowledge', 'senses', 'particularly', 'scenes', 'concerning', 'orton', 'halliwell', 'sets', 'particularly', 'flat', 'halliwell', \"'s\", 'murals', 'decorating', 'every', 'surface', 'terribly', 'well', 'done']\n",
      "\n",
      "Updated Tokens for Review 3:\n",
      "['thought', 'wonderful', 'way', 'spend', 'time', 'hot', 'summer', 'weekend', 'sitting', 'air', 'conditioned', 'theater', 'watching', 'light-hearted', 'comedy', 'plot', 'simplistic', 'dialogue', 'witty', 'characters', 'likable', 'even', 'well', 'suspected', 'serial', 'killer', 'may', 'disappointed', 'realize', 'match', 'point', '2', 'risk', 'addiction', 'thought', 'proof', 'woody', 'allen', 'still', 'fully', 'control', 'style', 'many', 'us', 'grown', 'love.', \"'d\", 'laughed', 'one', 'woody', \"'s\", 'comedies', 'years', 'dare', 'say', 'decade', \"'ve\", 'never', 'impressed', 'scarlet', 'johanson', 'managed', 'tone', '``', 'sexy', \"''\", 'image', 'jumped', 'right', 'average', 'spirited', 'young', 'woman.', 'may', 'crown', 'jewel', 'career', 'wittier', '``', 'devil', 'wears', 'prada', \"''\", 'interesting', '``', 'superman', \"''\", 'great', 'comedy', 'go', 'see', 'friends']\n",
      "\n",
      "Updated Tokens for Review 4:\n",
      "['basically', \"'s\", 'family', 'little', 'boy', 'jake', 'thinks', \"'s\", 'zombie', 'closet', 'parents', 'fighting', 'time.', 'movie', 'slower', 'soap', 'opera', '...', 'suddenly', 'jake', 'decides', 'become', 'rambo', 'kill', 'zombie.', 'ok', 'first', \"'re\", 'going', 'make', 'film', 'must', 'decide', 'thriller', 'drama', 'drama', 'movie', 'watchable', 'parents', 'divorcing', 'arguing', 'like', 'real', 'life', 'jake', 'closet', 'totally', 'ruins', 'film', 'expected', 'see', 'boogeyman', 'similar', 'movie', 'instead', 'watched', 'drama', 'meaningless', 'thriller', 'spots.', '3', '10', 'well', 'playing', 'parents', 'descent', 'dialogs', 'shots', 'jake', 'ignore']\n",
      "\n",
      "Updated Tokens for Review 5:\n",
      "['petter', 'mattei', \"'s\", '``', 'love', 'time', 'money', \"''\", 'visually', 'stunning', 'film', 'watch', 'mr.', 'mattei', 'offers', 'us', 'vivid', 'portrait', 'human', 'relations', 'movie', 'seems', 'telling', 'us', 'money', 'power', 'success', 'people', 'different', 'situations', 'encounter', 'variation', 'arthur', 'schnitzler', \"'s\", 'play', 'theme', 'director', 'transfers', 'action', 'present', 'time', 'new', 'york', 'different', 'characters', 'meet', 'connect', 'one', 'connected', 'one', 'way', 'another', 'next', 'person', 'one', 'seems', 'know', 'previous', 'point', 'contact', 'stylishly', 'film', 'sophisticated', 'luxurious', 'look', 'taken', 'see', 'people', 'live', 'world', 'live', 'habitat.', 'thing', 'one', 'gets', 'souls', 'picture', 'different', 'stages', 'loneliness', 'one', 'inhabits', 'big', 'city', 'exactly', 'best', 'place', 'human', 'relations', 'find', 'sincere', 'fulfillment', 'one', 'discerns', 'case', 'people', 'encounter.', 'acting', 'good', 'mr.', 'mattei', \"'s\", 'direction', 'steve', 'buscemi', 'rosario', 'dawson', 'carol', 'kane', 'michael', 'imperioli', 'adrian', 'grenier', 'rest', 'talented', 'cast', 'make', 'characters', 'come', 'alive.', 'wish', 'mr.', 'mattei', 'good', 'luck', 'await', 'anxiously', 'next', 'work']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Function to remove stopwords from tokens\n",
    "def remove_stopwords(tokens):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    return [token for token in tokens if token not in stop_words]\n",
    "\n",
    "# Apply the function to your 'tokens' column\n",
    "imdb_data['tokens'] = imdb_data['tokens'].apply(remove_stopwords)\n",
    "\n",
    "# Print the updated tokens for the first few rows\n",
    "for i in range(5):\n",
    "    print(f\"Updated Tokens for Review {i+1}:\")\n",
    "    print(imdb_data['tokens'][i])\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "38ca49ef-ca31-41df-be89-aa606af30b4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated Tokens for Review 44:\n",
      "['one', 'reviewers', 'mentioned', 'watching', 'one', 'oz', 'episode', \"'ll\", 'hooked', 'right', 'exactly', 'happened', 'me.', 'first', 'thing', 'struck', 'oz', 'unflinching', 'scenes', 'violence', 'set', 'right', 'word', 'go', 'trust', 'show', 'faint', 'hearted', 'timid', 'show', 'pulls', 'punches', 'regards', 'drugs', 'sex', 'violence', 'hardcore', 'classic', 'use', 'word.', 'called', 'oz', 'nickname', 'given', 'oswald', 'maximum', 'security', 'state', 'penitentary', 'focuses', 'mainly', 'emerald', 'city', 'experimental', 'section', 'prison', 'cells', 'glass', 'fronts', 'face', 'inwards', 'privacy', 'high', 'agenda', 'em', 'city', 'home', 'many', '..', 'aryans', 'muslims', 'gangstas', 'latinos', 'christians', 'italians', 'irish', '....', 'scuffles', 'death', 'stares', 'dodgy', 'dealings', 'shady', 'agreements', 'never', 'far', 'away.', 'would', 'say', 'main', 'appeal', 'show', 'due', 'fact', 'goes', 'shows', 'would', \"n't\", 'dare', 'forget', 'pretty', 'pictures', 'painted', 'mainstream', 'audiences', 'forget', 'charm', 'forget', 'romance', '...', 'oz', \"n't\", 'mess', 'around', 'first', 'episode', 'ever', 'saw', 'struck', 'nasty', 'surreal', 'could', \"n't\", 'say', 'ready', 'watched', 'developed', 'taste', 'oz', 'got', 'accustomed', 'high', 'levels', 'graphic', 'violence', 'violence', 'injustice', 'crooked', 'guards', \"'ll\", 'sold', 'nickel', 'inmates', \"'ll\", 'kill', 'order', 'get', 'away', 'well', 'mannered', 'middle', 'class', 'inmates', 'turned', 'prison', 'bitches', 'due', 'lack', 'street', 'skills', 'prison', 'experience', 'watching', 'oz', 'may', 'become', 'comfortable', 'uncomfortable', 'viewing', '....', 'thats', 'get', 'touch', 'darker', 'side']\n",
      "\n",
      "Updated Tokens for Review 45:\n",
      "['wonderful', 'little', 'production', 'filming', 'technique', 'unassuming-', 'old-time-bbc', 'fashion', 'gives', 'comforting', 'sometimes', 'discomforting', 'sense', 'realism', 'entire', 'piece', 'actors', 'extremely', 'well', 'chosen-', 'michael', 'sheen', '``', 'got', 'polari', \"''\", 'voices', 'pat', 'truly', 'see', 'seamless', 'editing', 'guided', 'references', 'williams', 'diary', 'entries', 'well', 'worth', 'watching', 'terrificly', 'written', 'performed', 'piece', 'masterful', 'production', 'one', 'great', 'master', \"'s\", 'comedy', 'life', 'realism', 'really', 'comes', 'home', 'little', 'things', 'fantasy', 'guard', 'rather', 'use', 'traditional', \"'dream\", 'techniques', 'remains', 'solid', 'disappears', 'plays', 'knowledge', 'senses', 'particularly', 'scenes', 'concerning', 'orton', 'halliwell', 'sets', 'particularly', 'flat', 'halliwell', \"'s\", 'murals', 'decorating', 'every', 'surface', 'terribly', 'well', 'done']\n",
      "\n",
      "Updated Tokens for Review 46:\n",
      "['thought', 'wonderful', 'way', 'spend', 'time', 'hot', 'summer', 'weekend', 'sitting', 'air', 'conditioned', 'theater', 'watching', 'light-hearted', 'comedy', 'plot', 'simplistic', 'dialogue', 'witty', 'characters', 'likable', 'even', 'well', 'suspected', 'serial', 'killer', 'may', 'disappointed', 'realize', 'match', 'point', 'two', 'risk', 'addiction', 'thought', 'proof', 'woody', 'allen', 'still', 'fully', 'control', 'style', 'many', 'us', 'grown', 'love.', \"'d\", 'laughed', 'one', 'woody', \"'s\", 'comedies', 'years', 'dare', 'say', 'decade', \"'ve\", 'never', 'impressed', 'scarlet', 'johanson', 'managed', 'tone', '``', 'sexy', \"''\", 'image', 'jumped', 'right', 'average', 'spirited', 'young', 'woman.', 'may', 'crown', 'jewel', 'career', 'wittier', '``', 'devil', 'wears', 'prada', \"''\", 'interesting', '``', 'superman', \"''\", 'great', 'comedy', 'go', 'see', 'friends']\n",
      "\n",
      "Updated Tokens for Review 47:\n",
      "['basically', \"'s\", 'family', 'little', 'boy', 'jake', 'thinks', \"'s\", 'zombie', 'closet', 'parents', 'fighting', 'time.', 'movie', 'slower', 'soap', 'opera', '...', 'suddenly', 'jake', 'decides', 'become', 'rambo', 'kill', 'zombie.', 'ok', 'first', \"'re\", 'going', 'make', 'film', 'must', 'decide', 'thriller', 'drama', 'drama', 'movie', 'watchable', 'parents', 'divorcing', 'arguing', 'like', 'real', 'life', 'jake', 'closet', 'totally', 'ruins', 'film', 'expected', 'see', 'boogeyman', 'similar', 'movie', 'instead', 'watched', 'drama', 'meaningless', 'thriller', 'spots.', 'three', 'ten', 'well', 'playing', 'parents', 'descent', 'dialogs', 'shots', 'jake', 'ignore']\n",
      "\n",
      "Updated Tokens for Review 48:\n",
      "['petter', 'mattei', \"'s\", '``', 'love', 'time', 'money', \"''\", 'visually', 'stunning', 'film', 'watch', 'mr.', 'mattei', 'offers', 'us', 'vivid', 'portrait', 'human', 'relations', 'movie', 'seems', 'telling', 'us', 'money', 'power', 'success', 'people', 'different', 'situations', 'encounter', 'variation', 'arthur', 'schnitzler', \"'s\", 'play', 'theme', 'director', 'transfers', 'action', 'present', 'time', 'new', 'york', 'different', 'characters', 'meet', 'connect', 'one', 'connected', 'one', 'way', 'another', 'next', 'person', 'one', 'seems', 'know', 'previous', 'point', 'contact', 'stylishly', 'film', 'sophisticated', 'luxurious', 'look', 'taken', 'see', 'people', 'live', 'world', 'live', 'habitat.', 'thing', 'one', 'gets', 'souls', 'picture', 'different', 'stages', 'loneliness', 'one', 'inhabits', 'big', 'city', 'exactly', 'best', 'place', 'human', 'relations', 'find', 'sincere', 'fulfillment', 'one', 'discerns', 'case', 'people', 'encounter.', 'acting', 'good', 'mr.', 'mattei', \"'s\", 'direction', 'steve', 'buscemi', 'rosario', 'dawson', 'carol', 'kane', 'michael', 'imperioli', 'adrian', 'grenier', 'rest', 'talented', 'cast', 'make', 'characters', 'come', 'alive.', 'wish', 'mr.', 'mattei', 'good', 'luck', 'await', 'anxiously', 'next', 'work']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import inflect \n",
    "\n",
    "def convert_numbers_to_text(tokens):\n",
    "    p = inflect.engine()\n",
    "    converted_tokens = []\n",
    "    \n",
    "    for token in tokens:\n",
    "        if token.isdigit():\n",
    "            text_representation = p.number_to_words(token)\n",
    "            converted_tokens.append(text_representation)\n",
    "        else:\n",
    "            converted_tokens.append(token)\n",
    "    \n",
    "    return converted_tokens\n",
    "\n",
    "# Apply the function to your 'tokens' column\n",
    "imdb_data['tokens'] = imdb_data['tokens'].apply(convert_numbers_to_text)\n",
    "\n",
    "# Print the updated tokens for the first few rows\n",
    "for i in range(5):\n",
    "    print(f\"Updated Tokens for Review {i+44}:\")\n",
    "    print(imdb_data['tokens'][i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c8d9d46a-3272-4618-84b5-6a30587c25e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated Tokens for Review 88:\n",
      "['one', 'reviewers', 'mentioned', 'watching', 'one', 'oz', 'episode', \"'ll\", 'hooked', 'right', 'exactly', 'happened', 'me.', 'first', 'thing', 'struck', 'oz', 'unflinching', 'scenes', 'violence', 'set', 'right', 'word', 'go', 'trust', 'show', 'faint', 'hearted', 'timid', 'show', 'pulls', 'punches', 'regards', 'drugs', 'sex', 'violence', 'hardcore', 'classic', 'use', 'word.', 'called', 'oz', 'nickname', 'given', 'oswald', 'maximum', 'security', 'state', 'penitentary', 'focuses', 'mainly', 'emerald', 'city', 'experimental', 'section', 'prison', 'cells', 'glass', 'fronts', 'face', 'inwards', 'privacy', 'high', 'agenda', 'em', 'city', 'home', 'many', '..', 'aryans', 'muslims', 'gangstas', 'latinos', 'christians', 'italians', 'irish', '....', 'scuffles', 'death', 'stares', 'dodgy', 'dealings', 'shady', 'agreements', 'never', 'far', 'away.', 'would', 'say', 'main', 'appeal', 'show', 'due', 'fact', 'goes', 'shows', 'would', \"n't\", 'dare', 'forget', 'pretty', 'pictures', 'painted', 'mainstream', 'audiences', 'forget', 'charm', 'forget', 'romance', '...', 'oz', \"n't\", 'mess', 'around', 'first', 'episode', 'ever', 'saw', 'struck', 'nasty', 'surreal', 'could', \"n't\", 'say', 'ready', 'watched', 'developed', 'taste', 'oz', 'got', 'accustomed', 'high', 'levels', 'graphic', 'violence', 'violence', 'injustice', 'crooked', 'guards', \"'ll\", 'sold', 'nickel', 'inmates', \"'ll\", 'kill', 'order', 'get', 'away', 'well', 'mannered', 'middle', 'class', 'inmates', 'turned', 'prison', 'bitches', 'due', 'lack', 'street', 'skills', 'prison', 'experience', 'watching', 'oz', 'may', 'become', 'comfortable', 'uncomfortable', 'viewing', '....', 'thats', 'get', 'touch', 'darker', 'side']\n",
      "\n",
      "Updated Tokens for Review 89:\n",
      "['wonderful', 'little', 'production', 'filming', 'technique', 'unassuming-', 'old-time-bbc', 'fashion', 'gives', 'comforting', 'sometimes', 'discomforting', 'sense', 'realism', 'entire', 'piece', 'actors', 'extremely', 'well', 'chosen-', 'michael', 'sheen', '``', 'got', 'polari', \"''\", 'voices', 'pat', 'truly', 'see', 'seamless', 'editing', 'guided', 'references', 'williams', 'diary', 'entries', 'well', 'worth', 'watching', 'terrificly', 'written', 'performed', 'piece', 'masterful', 'production', 'one', 'great', 'master', \"'s\", 'comedy', 'life', 'realism', 'really', 'comes', 'home', 'little', 'things', 'fantasy', 'guard', 'rather', 'use', 'traditional', \"'dream\", 'techniques', 'remains', 'solid', 'disappears', 'plays', 'knowledge', 'senses', 'particularly', 'scenes', 'concerning', 'orton', 'halliwell', 'sets', 'particularly', 'flat', 'halliwell', \"'s\", 'murals', 'decorating', 'every', 'surface', 'terribly', 'well', 'done']\n",
      "\n",
      "Updated Tokens for Review 90:\n",
      "['thought', 'wonderful', 'way', 'spend', 'time', 'hot', 'summer', 'weekend', 'sitting', 'air', 'conditioned', 'theater', 'watching', 'light-hearted', 'comedy', 'plot', 'simplistic', 'dialogue', 'witty', 'characters', 'likable', 'even', 'well', 'suspected', 'serial', 'killer', 'may', 'disappointed', 'realize', 'match', 'point', 'two', 'risk', 'addiction', 'thought', 'proof', 'woody', 'allen', 'still', 'fully', 'control', 'style', 'many', 'us', 'grown', 'love.', \"'d\", 'laughed', 'one', 'woody', \"'s\", 'comedies', 'years', 'dare', 'say', 'decade', \"'ve\", 'never', 'impressed', 'scarlet', 'johanson', 'managed', 'tone', '``', 'sexy', \"''\", 'image', 'jumped', 'right', 'average', 'spirited', 'young', 'woman.', 'may', 'crown', 'jewel', 'career', 'wittier', '``', 'devil', 'wears', 'prada', \"''\", 'interesting', '``', 'superman', \"''\", 'great', 'comedy', 'go', 'see', 'friends']\n",
      "\n",
      "Updated Tokens for Review 91:\n",
      "['basically', \"'s\", 'family', 'little', 'boy', 'jake', 'thinks', \"'s\", 'zombie', 'closet', 'parents', 'fighting', 'time.', 'movie', 'slower', 'soap', 'opera', '...', 'suddenly', 'jake', 'decides', 'become', 'rambo', 'kill', 'zombie.', 'ok', 'first', \"'re\", 'going', 'make', 'film', 'must', 'decide', 'thriller', 'drama', 'drama', 'movie', 'watchable', 'parents', 'divorcing', 'arguing', 'like', 'real', 'life', 'jake', 'closet', 'totally', 'ruins', 'film', 'expected', 'see', 'boogeyman', 'similar', 'movie', 'instead', 'watched', 'drama', 'meaningless', 'thriller', 'spots.', 'three', 'ten', 'well', 'playing', 'parents', 'descent', 'dialogs', 'shots', 'jake', 'ignore']\n",
      "\n",
      "Updated Tokens for Review 92:\n",
      "['petter', 'mattei', \"'s\", '``', 'love', 'time', 'money', \"''\", 'visually', 'stunning', 'film', 'watch', 'mr.', 'mattei', 'offers', 'us', 'vivid', 'portrait', 'human', 'relations', 'movie', 'seems', 'telling', 'us', 'money', 'power', 'success', 'people', 'different', 'situations', 'encounter', 'variation', 'arthur', 'schnitzler', \"'s\", 'play', 'theme', 'director', 'transfers', 'action', 'present', 'time', 'new', 'york', 'different', 'characters', 'meet', 'connect', 'one', 'connected', 'one', 'way', 'another', 'next', 'person', 'one', 'seems', 'know', 'previous', 'point', 'contact', 'stylishly', 'film', 'sophisticated', 'luxurious', 'look', 'taken', 'see', 'people', 'live', 'world', 'live', 'habitat.', 'thing', 'one', 'gets', 'souls', 'picture', 'different', 'stages', 'loneliness', 'one', 'inhabits', 'big', 'city', 'exactly', 'best', 'place', 'human', 'relations', 'find', 'sincere', 'fulfillment', 'one', 'discerns', 'case', 'people', 'encounter.', 'acting', 'good', 'mr.', 'mattei', \"'s\", 'direction', 'steve', 'buscemi', 'rosario', 'dawson', 'carol', 'kane', 'michael', 'imperioli', 'adrian', 'grenier', 'rest', 'talented', 'cast', 'make', 'characters', 'come', 'alive.', 'wish', 'mr.', 'mattei', 'good', 'luck', 'await', 'anxiously', 'next', 'work']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def remove_punctuation(tokens):\n",
    "    # Define a set of punctuation characters\n",
    "    punctuation_set = set(string.punctuation)\n",
    "    \n",
    "    # Filter out tokens that are not punctuation\n",
    "    filtered_tokens = [token for token in tokens if token not in punctuation_set]\n",
    "    \n",
    "    return filtered_tokens\n",
    "\n",
    "# Apply the function to your 'tokens' column\n",
    "imdb_data['tokens'] = imdb_data['tokens'].apply(remove_punctuation)\n",
    "\n",
    "# Print the updated tokens for the first few rows\n",
    "for i in range(5):\n",
    "    print(f\"Updated Tokens for Review {i+88}:\")\n",
    "    print(imdb_data['tokens'][i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fc839833-cb60-48eb-bb83-5cde4b9d2b8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\MITUL\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated Tokens for Review 67:\n",
      "['one', 'reviewer', 'mentioned', 'watching', 'one', 'oz', 'episode', \"'ll\", 'hooked', 'right', 'exactly', 'happened', 'me.', 'first', 'thing', 'struck', 'oz', 'unflinching', 'scene', 'violence', 'set', 'right', 'word', 'go', 'trust', 'show', 'faint', 'hearted', 'timid', 'show', 'pull', 'punch', 'regard', 'drug', 'sex', 'violence', 'hardcore', 'classic', 'use', 'word.', 'called', 'oz', 'nickname', 'given', 'oswald', 'maximum', 'security', 'state', 'penitentary', 'focus', 'mainly', 'emerald', 'city', 'experimental', 'section', 'prison', 'cell', 'glass', 'front', 'face', 'inwards', 'privacy', 'high', 'agenda', 'em', 'city', 'home', 'many', '..', 'aryan', 'muslim', 'gangsta', 'latino', 'christian', 'italian', 'irish', '....', 'scuffle', 'death', 'stare', 'dodgy', 'dealing', 'shady', 'agreement', 'never', 'far', 'away.', 'would', 'say', 'main', 'appeal', 'show', 'due', 'fact', 'go', 'show', 'would', \"n't\", 'dare', 'forget', 'pretty', 'picture', 'painted', 'mainstream', 'audience', 'forget', 'charm', 'forget', 'romance', '...', 'oz', \"n't\", 'mess', 'around', 'first', 'episode', 'ever', 'saw', 'struck', 'nasty', 'surreal', 'could', \"n't\", 'say', 'ready', 'watched', 'developed', 'taste', 'oz', 'got', 'accustomed', 'high', 'level', 'graphic', 'violence', 'violence', 'injustice', 'crooked', 'guard', \"'ll\", 'sold', 'nickel', 'inmate', \"'ll\", 'kill', 'order', 'get', 'away', 'well', 'mannered', 'middle', 'class', 'inmate', 'turned', 'prison', 'bitch', 'due', 'lack', 'street', 'skill', 'prison', 'experience', 'watching', 'oz', 'may', 'become', 'comfortable', 'uncomfortable', 'viewing', '....', 'thats', 'get', 'touch', 'darker', 'side']\n",
      "\n",
      "Updated Tokens for Review 68:\n",
      "['wonderful', 'little', 'production', 'filming', 'technique', 'unassuming-', 'old-time-bbc', 'fashion', 'give', 'comforting', 'sometimes', 'discomforting', 'sense', 'realism', 'entire', 'piece', 'actor', 'extremely', 'well', 'chosen-', 'michael', 'sheen', '``', 'got', 'polari', \"''\", 'voice', 'pat', 'truly', 'see', 'seamless', 'editing', 'guided', 'reference', 'williams', 'diary', 'entry', 'well', 'worth', 'watching', 'terrificly', 'written', 'performed', 'piece', 'masterful', 'production', 'one', 'great', 'master', \"'s\", 'comedy', 'life', 'realism', 'really', 'come', 'home', 'little', 'thing', 'fantasy', 'guard', 'rather', 'use', 'traditional', \"'dream\", 'technique', 'remains', 'solid', 'disappears', 'play', 'knowledge', 'sens', 'particularly', 'scene', 'concerning', 'orton', 'halliwell', 'set', 'particularly', 'flat', 'halliwell', \"'s\", 'mural', 'decorating', 'every', 'surface', 'terribly', 'well', 'done']\n",
      "\n",
      "Updated Tokens for Review 69:\n",
      "['thought', 'wonderful', 'way', 'spend', 'time', 'hot', 'summer', 'weekend', 'sitting', 'air', 'conditioned', 'theater', 'watching', 'light-hearted', 'comedy', 'plot', 'simplistic', 'dialogue', 'witty', 'character', 'likable', 'even', 'well', 'suspected', 'serial', 'killer', 'may', 'disappointed', 'realize', 'match', 'point', 'two', 'risk', 'addiction', 'thought', 'proof', 'woody', 'allen', 'still', 'fully', 'control', 'style', 'many', 'u', 'grown', 'love.', \"'d\", 'laughed', 'one', 'woody', \"'s\", 'comedy', 'year', 'dare', 'say', 'decade', \"'ve\", 'never', 'impressed', 'scarlet', 'johanson', 'managed', 'tone', '``', 'sexy', \"''\", 'image', 'jumped', 'right', 'average', 'spirited', 'young', 'woman.', 'may', 'crown', 'jewel', 'career', 'wittier', '``', 'devil', 'wear', 'prada', \"''\", 'interesting', '``', 'superman', \"''\", 'great', 'comedy', 'go', 'see', 'friend']\n",
      "\n",
      "Updated Tokens for Review 70:\n",
      "['basically', \"'s\", 'family', 'little', 'boy', 'jake', 'think', \"'s\", 'zombie', 'closet', 'parent', 'fighting', 'time.', 'movie', 'slower', 'soap', 'opera', '...', 'suddenly', 'jake', 'decides', 'become', 'rambo', 'kill', 'zombie.', 'ok', 'first', \"'re\", 'going', 'make', 'film', 'must', 'decide', 'thriller', 'drama', 'drama', 'movie', 'watchable', 'parent', 'divorcing', 'arguing', 'like', 'real', 'life', 'jake', 'closet', 'totally', 'ruin', 'film', 'expected', 'see', 'boogeyman', 'similar', 'movie', 'instead', 'watched', 'drama', 'meaningless', 'thriller', 'spots.', 'three', 'ten', 'well', 'playing', 'parent', 'descent', 'dialog', 'shot', 'jake', 'ignore']\n",
      "\n",
      "Updated Tokens for Review 71:\n",
      "['petter', 'mattei', \"'s\", '``', 'love', 'time', 'money', \"''\", 'visually', 'stunning', 'film', 'watch', 'mr.', 'mattei', 'offer', 'u', 'vivid', 'portrait', 'human', 'relation', 'movie', 'seems', 'telling', 'u', 'money', 'power', 'success', 'people', 'different', 'situation', 'encounter', 'variation', 'arthur', 'schnitzler', \"'s\", 'play', 'theme', 'director', 'transfer', 'action', 'present', 'time', 'new', 'york', 'different', 'character', 'meet', 'connect', 'one', 'connected', 'one', 'way', 'another', 'next', 'person', 'one', 'seems', 'know', 'previous', 'point', 'contact', 'stylishly', 'film', 'sophisticated', 'luxurious', 'look', 'taken', 'see', 'people', 'live', 'world', 'live', 'habitat.', 'thing', 'one', 'get', 'soul', 'picture', 'different', 'stage', 'loneliness', 'one', 'inhabits', 'big', 'city', 'exactly', 'best', 'place', 'human', 'relation', 'find', 'sincere', 'fulfillment', 'one', 'discerns', 'case', 'people', 'encounter.', 'acting', 'good', 'mr.', 'mattei', \"'s\", 'direction', 'steve', 'buscemi', 'rosario', 'dawson', 'carol', 'kane', 'michael', 'imperioli', 'adrian', 'grenier', 'rest', 'talented', 'cast', 'make', 'character', 'come', 'alive.', 'wish', 'mr.', 'mattei', 'good', 'luck', 'await', 'anxiously', 'next', 'work']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('wordnet')\n",
    "\n",
    "def lemmatize_tokens(tokens):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    return lemmatized_tokens\n",
    "\n",
    "imdb_data['tokens'] = imdb_data['tokens'].apply(lemmatize_tokens)\n",
    "\n",
    "# Print the updated tokens for the first few rows\n",
    "for i in range(5):\n",
    "    print(f\"Updated Tokens for Review {i+67}:\")\n",
    "    print(imdb_data['tokens'][i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3a64bbe7-93cf-491e-9fc9-267396df3e87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [one, reviewer, mentioned, watching, one, oz, ...\n",
       "1        [wonderful, little, production, filming, techn...\n",
       "2        [thought, wonderful, way, spend, time, hot, su...\n",
       "3        [basically, 's, family, little, boy, jake, thi...\n",
       "4        [petter, mattei, 's, ``, love, time, money, ''...\n",
       "                               ...                        \n",
       "49995    [thought, movie, right, good, job, n't, creati...\n",
       "49996    [bad, plot, bad, dialogue, bad, acting, idioti...\n",
       "49997    [catholic, taught, parochial, elementary, scho...\n",
       "49998    ['m, going, disagree, previous, comment, side,...\n",
       "49999    [one, expects, star, trek, movie, high, art, f...\n",
       "Name: tokens, Length: 50000, dtype: object"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_data['tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0d808c35-90a6-45d4-a331-bb1fef8355e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        positive\n",
       "1        positive\n",
       "2        positive\n",
       "3        negative\n",
       "4        positive\n",
       "           ...   \n",
       "49995    positive\n",
       "49996    negative\n",
       "49997    negative\n",
       "49998    negative\n",
       "49999    negative\n",
       "Name: sentiment, Length: 50000, dtype: object"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_data['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "895e381f-30e7-4c7e-9ca5-06943278f8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# Path to your .vec file\n",
    "model_path = 'NLP\\crawl-300d-2M.vec'\n",
    "\n",
    "# Load the model\n",
    "# This might take some time due to the size of the model\n",
    "model = KeyedVectors.load_word2vec_format(model_path, binary=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1f4ca4b8-77d8-4862-b756-bac2a74b9c3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        [-0.015188309, 0.01125789, -0.012918132, -0.07...\n",
      "1        [-0.054054197, 0.05932771, -0.026885541, -0.04...\n",
      "2        [-0.033824418, -0.08096747, 0.004375581, -0.06...\n",
      "3        [-0.048980013, -0.12747999, 0.032747142, -0.07...\n",
      "4        [-0.064749606, -0.0659632, -0.034083195, -0.05...\n",
      "                               ...                        \n",
      "49995    [-0.07586127, -0.10612625, 0.0048349965, -0.08...\n",
      "49996    [-0.020407274, -0.030718181, -0.01155091, -0.0...\n",
      "49997    [-0.011429501, -0.046025403, -0.111038536, -0....\n",
      "49998    [0.030093862, -0.036876317, -0.0221579, -0.124...\n",
      "49999    [-0.08149552, -0.095549256, 0.027171643, -0.12...\n",
      "Name: doc_vector, Length: 50000, dtype: object\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "# Function to create an average feature vector for a document\n",
    "def document_vector(doc):\n",
    "    # Remove out-of-vocabulary words\n",
    "    doc = [word for word in doc if word in model.key_to_index]\n",
    "    if not doc:\n",
    "        return np.zeros(model.vector_size)\n",
    "    # Compute the mean of the word vectors for the words in the document\n",
    "    return np.mean(model[doc], axis=0)\n",
    "\n",
    "# Apply the function to each document\n",
    "imdb_data['doc_vector'] = imdb_data['tokens'].apply(document_vector)\n",
    "\n",
    "# Now, `imdb_data['doc_vector']` contains the feature vectors for your documents\n",
    "print(imdb_data['doc_vector'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c522449a-f0cd-4db1-8c5d-0beca86957fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tokens</th>\n",
       "      <th>doc_vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>[one, reviewer, mentioned, watching, one, oz, ...</td>\n",
       "      <td>[-0.015188309, 0.01125789, -0.012918132, -0.07...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "      <td>[wonderful, little, production, filming, techn...</td>\n",
       "      <td>[-0.054054197, 0.05932771, -0.026885541, -0.04...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "      <td>[thought, wonderful, way, spend, time, hot, su...</td>\n",
       "      <td>[-0.033824418, -0.08096747, 0.004375581, -0.06...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>[basically, 's, family, little, boy, jake, thi...</td>\n",
       "      <td>[-0.048980013, -0.12747999, 0.032747142, -0.07...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "      <td>[petter, mattei, 's, ``, love, time, money, ''...</td>\n",
       "      <td>[-0.064749606, -0.0659632, -0.034083195, -0.05...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>I thought this movie did a down right good job...</td>\n",
       "      <td>positive</td>\n",
       "      <td>[thought, movie, right, good, job, n't, creati...</td>\n",
       "      <td>[-0.07586127, -0.10612625, 0.0048349965, -0.08...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>Bad plot, bad dialogue, bad acting, idiotic di...</td>\n",
       "      <td>negative</td>\n",
       "      <td>[bad, plot, bad, dialogue, bad, acting, idioti...</td>\n",
       "      <td>[-0.020407274, -0.030718181, -0.01155091, -0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>I am a Catholic taught in parochial elementary...</td>\n",
       "      <td>negative</td>\n",
       "      <td>[catholic, taught, parochial, elementary, scho...</td>\n",
       "      <td>[-0.011429501, -0.046025403, -0.111038536, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>I'm going to have to disagree with the previou...</td>\n",
       "      <td>negative</td>\n",
       "      <td>['m, going, disagree, previous, comment, side,...</td>\n",
       "      <td>[0.030093862, -0.036876317, -0.0221579, -0.124...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>No one expects the Star Trek movies to be high...</td>\n",
       "      <td>negative</td>\n",
       "      <td>[one, expects, star, trek, movie, high, art, f...</td>\n",
       "      <td>[-0.08149552, -0.095549256, 0.027171643, -0.12...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review sentiment  \\\n",
       "0      One of the other reviewers has mentioned that ...  positive   \n",
       "1      A wonderful little production. <br /><br />The...  positive   \n",
       "2      I thought this was a wonderful way to spend ti...  positive   \n",
       "3      Basically there's a family where a little boy ...  negative   \n",
       "4      Petter Mattei's \"Love in the Time of Money\" is...  positive   \n",
       "...                                                  ...       ...   \n",
       "49995  I thought this movie did a down right good job...  positive   \n",
       "49996  Bad plot, bad dialogue, bad acting, idiotic di...  negative   \n",
       "49997  I am a Catholic taught in parochial elementary...  negative   \n",
       "49998  I'm going to have to disagree with the previou...  negative   \n",
       "49999  No one expects the Star Trek movies to be high...  negative   \n",
       "\n",
       "                                                  tokens  \\\n",
       "0      [one, reviewer, mentioned, watching, one, oz, ...   \n",
       "1      [wonderful, little, production, filming, techn...   \n",
       "2      [thought, wonderful, way, spend, time, hot, su...   \n",
       "3      [basically, 's, family, little, boy, jake, thi...   \n",
       "4      [petter, mattei, 's, ``, love, time, money, ''...   \n",
       "...                                                  ...   \n",
       "49995  [thought, movie, right, good, job, n't, creati...   \n",
       "49996  [bad, plot, bad, dialogue, bad, acting, idioti...   \n",
       "49997  [catholic, taught, parochial, elementary, scho...   \n",
       "49998  ['m, going, disagree, previous, comment, side,...   \n",
       "49999  [one, expects, star, trek, movie, high, art, f...   \n",
       "\n",
       "                                              doc_vector  \n",
       "0      [-0.015188309, 0.01125789, -0.012918132, -0.07...  \n",
       "1      [-0.054054197, 0.05932771, -0.026885541, -0.04...  \n",
       "2      [-0.033824418, -0.08096747, 0.004375581, -0.06...  \n",
       "3      [-0.048980013, -0.12747999, 0.032747142, -0.07...  \n",
       "4      [-0.064749606, -0.0659632, -0.034083195, -0.05...  \n",
       "...                                                  ...  \n",
       "49995  [-0.07586127, -0.10612625, 0.0048349965, -0.08...  \n",
       "49996  [-0.020407274, -0.030718181, -0.01155091, -0.0...  \n",
       "49997  [-0.011429501, -0.046025403, -0.111038536, -0....  \n",
       "49998  [0.030093862, -0.036876317, -0.0221579, -0.124...  \n",
       "49999  [-0.08149552, -0.095549256, 0.027171643, -0.12...  \n",
       "\n",
       "[50000 rows x 4 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "eff1b2d4-7f0a-47dc-b0b2-b5ebf2b0d54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_data.to_csv('processed_imdb_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df81e39e-b145-4ca1-9a02-2d1f6299d29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c6e5858-27c2-4bd8-b5ed-89c50ea09658",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              review sentiment  \\\n",
      "0  One of the other reviewers has mentioned that ...  positive   \n",
      "1  A wonderful little production. <br /><br />The...  positive   \n",
      "2  I thought this was a wonderful way to spend ti...  positive   \n",
      "3  Basically there's a family where a little boy ...  negative   \n",
      "4  Petter Mattei's \"Love in the Time of Money\" is...  positive   \n",
      "\n",
      "                                              tokens  \\\n",
      "0  ['one', 'reviewer', 'mentioned', 'watching', '...   \n",
      "1  ['wonderful', 'little', 'production', 'filming...   \n",
      "2  ['thought', 'wonderful', 'way', 'spend', 'time...   \n",
      "3  ['basically', \"'s\", 'family', 'little', 'boy',...   \n",
      "4  ['petter', 'mattei', \"'s\", '``', 'love', 'time...   \n",
      "\n",
      "                                          doc_vector  \n",
      "0  [-1.51883094e-02  1.12578897e-02 -1.29181324e-...  \n",
      "1  [-5.40541969e-02  5.93277104e-02 -2.68855412e-...  \n",
      "2  [-3.38244177e-02 -8.09674710e-02  4.37558116e-...  \n",
      "3  [-4.89800125e-02 -1.27479985e-01  3.27471420e-...  \n",
      "4  [-6.47496060e-02 -6.59632012e-02 -3.40831950e-...  \n"
     ]
    }
   ],
   "source": [
    "# Load the IMDb dataset from a CSV file\n",
    "imdb_data = pd.read_csv('NLP/processed_imdb_data.csv')\n",
    "\n",
    "# Explore the dataset\n",
    "print(imdb_data.head())  # Display the first few rows of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef10c214-6ad3-4f57-9b54-45dd39db9c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=imdb_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dd4352a5-11c6-478d-a0cd-b5a897cc481a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features and Labels\n",
    "X = np.array(data['doc_vector'].tolist())\n",
    "y = data['sentiment'].apply(lambda x: 1 if x == 'positive' else 0).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a1118e1f-5596-4114-b351-d5caf6ee7344",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [-1.51883094e-02  1.12578897e-02 -1.29181324e-...\n",
       "1        [-5.40541969e-02  5.93277104e-02 -2.68855412e-...\n",
       "2        [-3.38244177e-02 -8.09674710e-02  4.37558116e-...\n",
       "3        [-4.89800125e-02 -1.27479985e-01  3.27471420e-...\n",
       "4        [-6.47496060e-02 -6.59632012e-02 -3.40831950e-...\n",
       "                               ...                        \n",
       "49995    [-7.58612677e-02 -1.06126249e-01  4.83499654e-...\n",
       "49996    [-0.02040727 -0.03071818 -0.01155091 -0.077592...\n",
       "49997    [-1.14295008e-02 -4.60254028e-02 -1.11038536e-...\n",
       "49998    [ 3.00938617e-02 -3.68763171e-02 -2.21579000e-...\n",
       "49999    [-8.14955235e-02 -9.55492556e-02  2.71716435e-...\n",
       "Name: doc_vector, Length: 50000, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"doc_vector\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5a874c84-f5e0-4a3e-a157-5e8a3acf9762",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parse_vector function\n",
    "def parse_vector(s):\n",
    "    # Removing brackets and splitting by space\n",
    "    numbers = s.replace('[', '').replace(']', '').split()\n",
    "    # Converting each number to float\n",
    "    return [float(x) for x in numbers]\n",
    "\n",
    "# Apply the function to the 'doc_vector' column\n",
    "data['doc_vector'] = data['doc_vector'].apply(parse_vector)\n",
    "\n",
    "# Convert the list of vectors to a NumPy array\n",
    "doc_vectors = np.array(data['doc_vector'].tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "651e03db-6e9a-46b2-bbc8-2df65398f72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(doc_vectors, y, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e3b28eda-8566-4391-aa21-874da8d146dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert arrays to PyTorch tensors\n",
    "# Convert to PyTorch tensors\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "X_val = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val = torch.tensor(y_val, dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "11a4c415-0e03-4ddd-a58c-a54c5b8e1d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a PyTorch Dataset\n",
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "45b52a9d-55a4-4e10-9f07-252874caaf0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "# Create DataLoader for both training and testing sets\n",
    "train_dataset = SentimentDataset(X_train, y_train)\n",
    "test_dataset = SentimentDataset(X_test, y_test)\n",
    "val_loader = DataLoader(TensorDataset(X_val, y_val), batch_size=64, shuffle=False)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bc22bc96-37c2-4b38-a4e6-a8217e54fcd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SentimentLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim=1, n_layers=2, bidirectional=True, dropout=0.5):\n",
    "        super(SentimentLSTM, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=n_layers, bidirectional=bidirectional, dropout=(dropout if n_layers > 1 else 0))\n",
    "        self.fc = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.bidirectional = bidirectional  # Define bidirectional here\n",
    "\n",
    "    def forward(self, text):\n",
    "        embedded = self.dropout(self.embedding(text))\n",
    "        _, (hidden, _) = self.lstm(embedded)\n",
    "        if self.bidirectional:  # Use self.bidirectional here\n",
    "            hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1))\n",
    "        else:\n",
    "            hidden = self.dropout(hidden[-1,:,:])\n",
    "        output = self.fc(hidden).squeeze()\n",
    "        return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ce019231-e460-44d1-be66-78d6d203345b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 147326\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "data['tokens'] = data['tokens'].apply(ast.literal_eval)\n",
    "\n",
    "# Flatten all lists into a single list containing all tokens\n",
    "all_tokens = [token for sublist in data['tokens'] for token in sublist]\n",
    "\n",
    "# Determine the unique tokens\n",
    "unique_tokens = set(all_tokens)\n",
    "\n",
    "# The vocabulary size is the number of unique tokens\n",
    "vocab_size = len(unique_tokens)\n",
    "\n",
    "# Printing the vocabulary size\n",
    "print(f'Vocabulary size: {vocab_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "faa120b5-9057-46d6-93aa-d788250d373b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_size = 300\n",
    "# hidden_size = 128 \n",
    "# lstm_units = 50\n",
    "# dense_units = 32\n",
    "num_classes=1\n",
    "# vocab_size = 147326  # Adjust based on your vocab\n",
    "# embedding_dim = 100\n",
    "# hidden_dim = 256 # dimension of the LSTM hidden state\n",
    "# output_dim = 1\n",
    "# n_layers = 2\n",
    "# bidirectional = True\n",
    "# dropout = 0.5\n",
    "input_size = 300\n",
    "lstm_units = 50\n",
    "num_epochs = 32\n",
    "output_dim = 1  \n",
    "model = SentimentLSTM(vocab_size, input_size, lstm_units, output_dim=output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c2331d69-a02a-4231-b0e0-90f0039aa9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam  # Import the Adam optimizer\n",
    "\n",
    "# Loss and optimizer\n",
    "optimizer = Adam(model.parameters())  # Use the Adam optimizer\n",
    "criterion = nn.BCEWithLogitsLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c96d4a51-c7ee-432f-b111-07e30cb592f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9f2c4cd3-4e8b-4aeb-8d09-d8566d157288",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e7f2dd5-1b14-44c5-ae55-d258c7b1c5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import torch\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# # Load the IMDb dataset from a CSV file\n",
    "# imdb_data = pd.read_csv('NLP/processed_imdb_data.csv')\n",
    "# # Explore the dataset\n",
    "# print(imdb_data.head())  # Display the first few rows of the dataset\n",
    "# data=imdb_data\n",
    "# # Features and Labels\n",
    "# X = np.array(data['doc_vector'].tolist())\n",
    "# y = data['sentiment'].apply(lambda x: 1 if x == 'positive' else 0).values\n",
    "# # Define the parse_vector function\n",
    "# def parse_vector(s):\n",
    "#     # Removing brackets and splitting by space\n",
    "#     numbers = s.replace('[', '').replace(']', '').split()\n",
    "#     # Converting each number to float\n",
    "#     return [float(x) for x in numbers]\n",
    "# # Apply the function to the 'doc_vector' column\n",
    "# data['doc_vector'] = data['doc_vector'].apply(parse_vector)\n",
    "# # Convert the list of vectors to a NumPy array\n",
    "# doc_vectors = np.array(data['doc_vector'].tolist())\n",
    "# # Split the dataset\n",
    "# X_train, X_temp, y_train, y_temp = train_test_split(doc_vectors, y, test_size=0.3, random_state=42)\n",
    "# X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "# X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "# y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "# X_val = torch.tensor(X_val, dtype=torch.float32)\n",
    "# y_val = torch.tensor(y_val, dtype=torch.float32)\n",
    "# X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "# y_test = torch.tensor(y_test, dtype=torch.float32)\n",
    "# # Define a PyTorch Dataset\n",
    "# class SentimentDataset(Dataset):\n",
    "#     def __init__(self, features, labels):\n",
    "#         self.features = features\n",
    "#         self.labels = labels\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.features)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         return self.features[idx], self.labels[idx]\n",
    "# from torch.utils.data import DataLoader, TensorDataset\n",
    "# # Create DataLoader for both training and testing sets\n",
    "# train_dataset = SentimentDataset(X_train, y_train)\n",
    "# test_dataset = SentimentDataset(X_test, y_test)\n",
    "# val_loader = DataLoader(TensorDataset(X_val, y_val), batch_size=64, shuffle=False)\n",
    "# train_loader = DataLoader(train_dataset, batch_size=64, shuffle=False)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "# class SentimentLSTM(nn.Module):\n",
    "#     def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim=1, n_layers=2, bidirectional=True, dropout=0.5):\n",
    "#         super(SentimentLSTM, self).__init__()\n",
    "#         self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "#         self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=n_layers, bidirectional=bidirectional, dropout=(dropout if n_layers > 1 else 0))\n",
    "#         self.fc = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)\n",
    "#         self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "#     def forward(self, text):\n",
    "#         embedded = self.dropout(self.embedding(text))\n",
    "#         _, (hidden, _) = self.lstm(embedded)\n",
    "#         if bidirectional:\n",
    "#             hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1))\n",
    "#         else:\n",
    "#             hidden = self.dropout(hidden[-1,:,:])\n",
    "#         output = self.fc(hidden).squeeze()\n",
    "#         return output\n",
    "# import ast\n",
    "# data['tokens'] = data['tokens'].apply(ast.literal_eval)\n",
    "\n",
    "# # Flatten all lists into a single list containing all tokens\n",
    "# all_tokens = [token for sublist in data['tokens'] for token in sublist]\n",
    "\n",
    "# # Determine the unique tokens\n",
    "# unique_tokens = set(all_tokens)\n",
    "\n",
    "# # The vocabulary size is the number of unique tokens\n",
    "# vocab_size = len(unique_tokens)\n",
    "\n",
    "# # Printing the vocabulary size\n",
    "# print(f'Vocabulary size: {vocab_size}')\n",
    "# # input_size = 300\n",
    "# # hidden_size = 128 \n",
    "# # lstm_units = 50\n",
    "# # dense_units = 32\n",
    "# num_classes=1\n",
    "# vocab_size = 147326  # Adjust based on your vocab\n",
    "# embedding_dim = 100\n",
    "# hidden_dim = 256 # dimension of the LSTM hidden state\n",
    "# output_dim = 1\n",
    "# n_layers = 2\n",
    "# bidirectional = True\n",
    "# dropout = 0.5\n",
    "# model = SentimentLSTM(vocab_size, embedding_dim, hidden_dim, output_dim, n_layers,\n",
    "#                       bidirectional, dropout)\n",
    "# from torch.optim import Adam  # Import the Adam optimizer\n",
    "\n",
    "# # Loss and optimizer\n",
    "# optimizer = Adam(model.parameters())  # Use the Adam optimizer\n",
    "# criterion = nn.BCEWithLogitsLoss()\n",
    "# device = torch.device(\"cuda\")\n",
    "# model = model.to(device)\n",
    "# criterion = criterion.to(device)\n",
    "# from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "# import numpy as np\n",
    "\n",
    "# num_epochs = 32\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     model.train()\n",
    "#     predictions = []\n",
    "#     true_labels = []\n",
    "    \n",
    "#     for features, labels in train_loader:\n",
    "#         features, labels = features.to(device), labels.to(device)\n",
    "#         features = features.long()\n",
    "#         optimizer.zero_grad()\n",
    "#         outputs = model(features).squeeze()\n",
    "        \n",
    "#         # Ensure labels are float and match output shape\n",
    "#         labels = labels.float()\n",
    "#         loss = criterion(outputs, labels)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "        \n",
    "#         # Convert outputs to binary predictions\n",
    "#         preds = torch.round(torch.sigmoid(outputs))\n",
    "#         predictions.extend(preds.detach().cpu().numpy())\n",
    "#         true_labels.extend(labels.detach().cpu().numpy())\n",
    "        \n",
    "#     accuracy = accuracy_score(true_labels, predictions)\n",
    "#     precision, recall, f1, _ = precision_recall_fscore_support(true_labels, predictions, average='binary')\n",
    "\n",
    "#     print(f'Training - Epoch [{epoch+1}/{num_epochs}]')\n",
    "#     print(f'Loss: {loss.item():.4f}')  # Print loss for the last batch\n",
    "#     print(f'Accuracy:   {accuracy:.4f}')\n",
    "#     print(f'Precision:  {precision:.4f}')\n",
    "#     print(f'Recall:     {recall:.4f}')\n",
    "#     print(f'F1 Score:   {f1:.4f}')\n",
    "\n",
    "#     # Validation loop\n",
    "#     model.eval()  # Set the model to evaluation mode\n",
    "#     val_predictions = []\n",
    "#     val_true_labels = []\n",
    "    \n",
    "#     for features, labels in val_loader:\n",
    "#         features, labels = features.to(device), labels.to(device)\n",
    "#         features = features.long()\n",
    "    \n",
    "#         # Forward pass\n",
    "#         val_outputs = model(features).squeeze()\n",
    "#         val_loss = criterion(val_outputs, labels)\n",
    "    \n",
    "#         # Convert outputs to binary predictions\n",
    "#         val_preds = torch.round(torch.sigmoid(val_outputs))\n",
    "#         val_predictions.extend(val_preds.detach().cpu().numpy())\n",
    "#         val_true_labels.extend(labels.detach().cpu().numpy())\n",
    "    \n",
    "#     val_accuracy = accuracy_score(val_true_labels, val_predictions)\n",
    "#     val_precision, val_recall, val_f1, _ = precision_recall_fscore_support(val_true_labels, val_predictions, average='binary')\n",
    "    \n",
    "#     # Print validation metrics\n",
    "#     print(f'Validation - Epoch [{epoch+1}/{num_epochs}]')\n",
    "#     print(f'Loss: {val_loss.item():.4f}')  # Print loss for the last batch\n",
    "#     print(f'Accuracy:   {val_accuracy:.4f}')\n",
    "#     print(f'Precision:  {val_precision:.4f}')\n",
    "#     print(f'Recall:     {val_recall:.4f}')\n",
    "#     print(f'F1:         {val_f1:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b9605b-564a-4b47-8191-03bb43bd2123",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import numpy as np\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "    \n",
    "    for features, labels in train_loader:\n",
    "        features, labels = features.to(device), labels.to(device)\n",
    "        features = features.long()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(features).squeeze()\n",
    "        \n",
    "        # Ensure labels are float and match output shape\n",
    "        labels = labels.float()\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Convert outputs to binary predictions\n",
    "        preds = torch.round(torch.sigmoid(outputs))\n",
    "        predictions.extend(preds.detach().cpu().numpy())\n",
    "        true_labels.extend(labels.detach().cpu().numpy())\n",
    "        \n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(true_labels, predictions, average='binary')\n",
    "\n",
    "    print(f'Training - Epoch [{epoch+1}/{num_epochs}]')\n",
    "    print(f'Loss: {loss.item():.4f}')  # Print loss for the last batch\n",
    "    print(f'Accuracy:   {accuracy:.4f}')\n",
    "    print(f'Precision:  {precision:.4f}')\n",
    "    print(f'Recall:     {recall:.4f}')\n",
    "    print(f'F1 Score:   {f1:.4f}')\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    val_predictions = []\n",
    "    val_true_labels = []\n",
    "    \n",
    "    for features, labels in val_loader:\n",
    "        features, labels = features.to(device), labels.to(device)\n",
    "        features = features.long()\n",
    "    \n",
    "        # Forward pass\n",
    "        val_outputs = model(features).squeeze()\n",
    "        val_loss = criterion(val_outputs, labels)\n",
    "    \n",
    "        # Convert outputs to binary predictions\n",
    "        val_preds = torch.round(torch.sigmoid(val_outputs))\n",
    "        val_predictions.extend(val_preds.detach().cpu().numpy())\n",
    "        val_true_labels.extend(labels.detach().cpu().numpy())\n",
    "    \n",
    "    val_accuracy = accuracy_score(val_true_labels, val_predictions)\n",
    "    val_precision, val_recall, val_f1, _ = precision_recall_fscore_support(val_true_labels, val_predictions, average='binary')\n",
    "    \n",
    "    # Print validation metrics\n",
    "    print(f'Validation - Epoch [{epoch+1}/{num_epochs}]')\n",
    "    print(f'Loss: {val_loss.item():.4f}')  # Print loss for the last batch\n",
    "    print(f'Accuracy:   {val_accuracy:.4f}')\n",
    "    print(f'Precision:  {val_precision:.4f}')\n",
    "    print(f'Recall:     {val_recall:.4f}')\n",
    "    print(f'F1:         {val_f1:.4f}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7713de-3431-4eb6-b1c2-7646c5d7d7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Load the IMDb dataset from a CSV file\n",
    "imdb_data = pd.read_csv('NLP/processed_imdb_data.csv')\n",
    "\n",
    "# Define the parse_vector function\n",
    "def parse_vector(s):\n",
    "    # Removing brackets and splitting by space\n",
    "    numbers = s.replace('[', '').replace(']', '').split()\n",
    "    # Converting each number to float\n",
    "    return [float(x) for x in numbers]\n",
    "\n",
    "# Apply the function to the 'doc_vector' column\n",
    "imdb_data['doc_vector'] = imdb_data['doc_vector'].apply(parse_vector)\n",
    "\n",
    "# Convert the list of vectors to a NumPy array\n",
    "doc_vectors = np.array(imdb_data['doc_vector'].tolist())\n",
    "\n",
    "# Features and Labels\n",
    "X = doc_vectors\n",
    "y = imdb_data['sentiment'].apply(lambda x: 1 if x == 'positive' else 0).values\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "X_val = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val = torch.tensor(y_val, dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "# Define a PyTorch Dataset\n",
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.labels[idx]\n",
    "\n",
    "# Create DataLoader for both training and testing sets\n",
    "train_dataset = SentimentDataset(X_train, y_train)\n",
    "test_dataset = SentimentDataset(X_test, y_test)\n",
    "val_loader = DataLoader(SentimentDataset(X_val, y_val), batch_size=64, shuffle=False)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "class SentimentLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, lstm_units, dense_units, num_classes):\n",
    "        super(SentimentLSTM, self).__init__()\n",
    "        self.embedding_dim = input_size\n",
    "        self.hidden_dim = hidden_size\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers=lstm_units, batch_first=True)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_size, dense_units),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(dense_units, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        out = self.fc(lstm_out[:, -1, :])  # Get the output from the last time step\n",
    "        return out\n",
    "\n",
    "# Define hyperparameters\n",
    "input_size = 300  # Adjust based on your input data\n",
    "hidden_size = 300  # Adjust based on your requirements\n",
    "lstm_units = 2\n",
    "dense_units = 32\n",
    "num_classes = 1\n",
    "\n",
    "# Create the model\n",
    "model = SentimentLSTM(input_size, hidden_size, lstm_units, dense_units, num_classes)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "num_epochs = 32\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "\n",
    "    for features, labels in train_loader:\n",
    "        features, labels = features.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(features)\n",
    "\n",
    "        # Ensure labels are float and match output shape\n",
    "        labels = labels.view(-1, 1).float()\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Convert outputs to binary predictions\n",
    "        preds = torch.round(torch.sigmoid(outputs))\n",
    "        predictions.extend(preds.detach().cpu().numpy())\n",
    "        true_labels.extend(labels.detach().cpu().numpy())\n",
    "\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(true_labels, predictions, average='binary')\n",
    "\n",
    "    print(f'Training - Epoch [{epoch+1}/{num_epochs}]')\n",
    "    print(f'Loss: {loss.item():.4f}')  # Print loss for the last batch\n",
    "    print(f'Accuracy:   {accuracy:.4f}')\n",
    "    print(f'Precision:  {precision:.4f}')\n",
    "    print(f'Recall:     {recall:.4f}')\n",
    "    print(f'F1 Score:   {f1:.4f}')\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    val_predictions = []\n",
    "    val_true_labels = []\n",
    "\n",
    "    for features, labels in val_loader:\n",
    "        features, labels = features.to(device), labels.to(device)\n",
    "        val_outputs = model(features)\n",
    "\n",
    "        # Ensure labels are float and match output shape\n",
    "        labels = labels.view(-1, 1).float()\n",
    "\n",
    "        # Convert outputs to binary predictions\n",
    "        val_preds = torch.round(torch.sigmoid(val_outputs))\n",
    "        val_predictions.extend(val_preds.detach().cpu().numpy())\n",
    "        val_true_labels.extend(labels.detach().cpu().numpy())\n",
    "\n",
    "    val_accuracy = accuracy_score(val_true_labels, val_predictions)\n",
    "    val_precision, val_recall, val_f1, _ = precision_recall_fscore_support(val_true_labels, val_predictions, average='binary')\n",
    "\n",
    "    # Print validation metrics\n",
    "    print(f'Validation - Epoch [{epoch+1}/{num_epochs}]')\n",
    "    print(f'Loss: {loss.item():.4f}')  # Print loss for the last batch\n",
    "    print(f'Accuracy:   {val_accuracy:.4f}')\n",
    "    print(f'Precision:  {val_precision:.4f}')\n",
    "    print(f'Recall:     {val_recall:.4f}')\n",
    "    print(f'F1:         {val_f1:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3861da97-9584-4915-8761-ec66c8e74b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Load the IMDb dataset from a CSV file\n",
    "imdb_data = pd.read_csv('NLP/processed_imdb_data.csv')\n",
    "\n",
    "# Define the parse_vector function\n",
    "def parse_vector(s):\n",
    "    # Removing brackets and splitting by space\n",
    "    numbers = s.replace('[', '').replace(']', '').split()\n",
    "    # Converting each number to float\n",
    "    return [float(x) for x in numbers]\n",
    "\n",
    "# Apply the function to the 'doc_vector' column\n",
    "imdb_data['doc_vector'] = imdb_data['doc_vector'].apply(parse_vector)\n",
    "\n",
    "# Convert the list of vectors to a NumPy array\n",
    "doc_vectors = np.array(imdb_data['doc_vector'].tolist())\n",
    "\n",
    "# Features and Labels\n",
    "X = doc_vectors\n",
    "y = imdb_data['sentiment'].apply(lambda x: 1 if x == 'positive' else 0).values\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "X_val = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val = torch.tensor(y_val, dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "# Define a PyTorch Dataset\n",
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.labels[idx]\n",
    "\n",
    "# Create DataLoader for both training and testing sets\n",
    "train_dataset = SentimentDataset(X_train, y_train)\n",
    "test_dataset = SentimentDataset(X_test, y_test)\n",
    "val_loader = DataLoader(SentimentDataset(X_val, y_val), batch_size=64, shuffle=False)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6adcf51b-a604-48dc-9d11-44a25832b866",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class BiLSTMSentiment(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(BiLSTMSentiment, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        # Bidirectional LSTM layer\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, bidirectional=True)\n",
    "        # Fully connected layer that outputs the binary classification\n",
    "        self.fc = nn.Linear(hidden_size * 2, num_classes)  # Multiply by 2 for bidirectionality\n",
    "\n",
    "    def forward(self, x):\n",
    "    # Reshape x to [batch_size, 1, features] to add a sequence length of 1\n",
    "        x = x.unsqueeze(1)  # Adds a sequence length of 1\n",
    "    \n",
    "    # Initialize hidden state and cell state\n",
    "        h0 = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_size).to(x.device)\n",
    "    \n",
    "    # Forward propagate LSTM\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "    \n",
    "    # Decode the hidden state of the last time step\n",
    "        out = self.fc(out[:, -1, :])\n",
    "    \n",
    "        return out\n",
    "\n",
    "# Assuming the input_size is the dimensionality of your document vectors\n",
    "input_size = 300  # This should match the size of your document vectors\n",
    "hidden_size = 128  # You can adjust this\n",
    "num_layers = 2  # Number of LSTM layers\n",
    "num_classes = 1  # For binary classification\n",
    "\n",
    "model = BiLSTMSentiment(input_size, hidden_size, num_layers, num_classes)\n",
    "\n",
    "# Moving model to the appropriate device\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "# Training hyperparameters\n",
    "num_epochs = 32\n",
    "learning_rate = 0.0001\n",
    "\n",
    "# You can adjust the number of epochs and learning rate based on your training requirements\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2f342aab-4ee9-4c09-907d-1e5367d40eb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training - Epoch [1/32]\n",
      "Loss: 0.1983\n",
      "Accuracy:   0.8246\n",
      "Precision:  0.8369\n",
      "Recall:     0.8041\n",
      "F1 Score:   0.8202\n",
      "Validation - Epoch [1/32]\n",
      "Loss: 0.1983\n",
      "Accuracy:   0.8623\n",
      "Precision:  0.8682\n",
      "Recall:     0.8594\n",
      "F1:         0.8638\n",
      "Training - Epoch [2/32]\n",
      "Loss: 0.1966\n",
      "Accuracy:   0.8609\n",
      "Precision:  0.8605\n",
      "Recall:     0.8599\n",
      "F1 Score:   0.8602\n",
      "Validation - Epoch [2/32]\n",
      "Loss: 0.1966\n",
      "Accuracy:   0.8633\n",
      "Precision:  0.8713\n",
      "Recall:     0.8578\n",
      "F1:         0.8645\n",
      "Training - Epoch [3/32]\n",
      "Loss: 0.1989\n",
      "Accuracy:   0.8627\n",
      "Precision:  0.8620\n",
      "Recall:     0.8621\n",
      "F1 Score:   0.8620\n",
      "Validation - Epoch [3/32]\n",
      "Loss: 0.1989\n",
      "Accuracy:   0.8636\n",
      "Precision:  0.8686\n",
      "Recall:     0.8620\n",
      "F1:         0.8653\n",
      "Training - Epoch [4/32]\n",
      "Loss: 0.2004\n",
      "Accuracy:   0.8643\n",
      "Precision:  0.8627\n",
      "Recall:     0.8648\n",
      "F1 Score:   0.8638\n",
      "Validation - Epoch [4/32]\n",
      "Loss: 0.2004\n",
      "Accuracy:   0.8633\n",
      "Precision:  0.8683\n",
      "Recall:     0.8617\n",
      "F1:         0.8650\n",
      "Training - Epoch [5/32]\n",
      "Loss: 0.1997\n",
      "Accuracy:   0.8655\n",
      "Precision:  0.8641\n",
      "Recall:     0.8658\n",
      "F1 Score:   0.8650\n",
      "Validation - Epoch [5/32]\n",
      "Loss: 0.1997\n",
      "Accuracy:   0.8636\n",
      "Precision:  0.8702\n",
      "Recall:     0.8599\n",
      "F1:         0.8650\n",
      "Training - Epoch [6/32]\n",
      "Loss: 0.1975\n",
      "Accuracy:   0.8670\n",
      "Precision:  0.8651\n",
      "Recall:     0.8679\n",
      "F1 Score:   0.8665\n",
      "Validation - Epoch [6/32]\n",
      "Loss: 0.1975\n",
      "Accuracy:   0.8655\n",
      "Precision:  0.8726\n",
      "Recall:     0.8609\n",
      "F1:         0.8667\n",
      "Training - Epoch [7/32]\n",
      "Loss: 0.1943\n",
      "Accuracy:   0.8685\n",
      "Precision:  0.8659\n",
      "Recall:     0.8706\n",
      "F1 Score:   0.8682\n",
      "Validation - Epoch [7/32]\n",
      "Loss: 0.1943\n",
      "Accuracy:   0.8663\n",
      "Precision:  0.8758\n",
      "Recall:     0.8586\n",
      "F1:         0.8671\n",
      "Training - Epoch [8/32]\n",
      "Loss: 0.1903\n",
      "Accuracy:   0.8694\n",
      "Precision:  0.8669\n",
      "Recall:     0.8712\n",
      "F1 Score:   0.8691\n",
      "Validation - Epoch [8/32]\n",
      "Loss: 0.1903\n",
      "Accuracy:   0.8668\n",
      "Precision:  0.8771\n",
      "Recall:     0.8580\n",
      "F1:         0.8675\n",
      "Training - Epoch [9/32]\n",
      "Loss: 0.1859\n",
      "Accuracy:   0.8707\n",
      "Precision:  0.8680\n",
      "Recall:     0.8727\n",
      "F1 Score:   0.8704\n",
      "Validation - Epoch [9/32]\n",
      "Loss: 0.1859\n",
      "Accuracy:   0.8663\n",
      "Precision:  0.8768\n",
      "Recall:     0.8573\n",
      "F1:         0.8669\n",
      "Training - Epoch [10/32]\n",
      "Loss: 0.1815\n",
      "Accuracy:   0.8718\n",
      "Precision:  0.8687\n",
      "Recall:     0.8744\n",
      "F1 Score:   0.8716\n",
      "Validation - Epoch [10/32]\n",
      "Loss: 0.1815\n",
      "Accuracy:   0.8675\n",
      "Precision:  0.8787\n",
      "Recall:     0.8575\n",
      "F1:         0.8680\n",
      "Training - Epoch [11/32]\n",
      "Loss: 0.1771\n",
      "Accuracy:   0.8727\n",
      "Precision:  0.8697\n",
      "Recall:     0.8751\n",
      "F1 Score:   0.8724\n",
      "Validation - Epoch [11/32]\n",
      "Loss: 0.1771\n",
      "Accuracy:   0.8680\n",
      "Precision:  0.8799\n",
      "Recall:     0.8573\n",
      "F1:         0.8684\n",
      "Training - Epoch [12/32]\n",
      "Loss: 0.1729\n",
      "Accuracy:   0.8747\n",
      "Precision:  0.8711\n",
      "Recall:     0.8781\n",
      "F1 Score:   0.8745\n",
      "Validation - Epoch [12/32]\n",
      "Loss: 0.1729\n",
      "Accuracy:   0.8688\n",
      "Precision:  0.8803\n",
      "Recall:     0.8586\n",
      "F1:         0.8693\n",
      "Training - Epoch [13/32]\n",
      "Loss: 0.1691\n",
      "Accuracy:   0.8752\n",
      "Precision:  0.8719\n",
      "Recall:     0.8782\n",
      "F1 Score:   0.8750\n",
      "Validation - Epoch [13/32]\n",
      "Loss: 0.1691\n",
      "Accuracy:   0.8688\n",
      "Precision:  0.8807\n",
      "Recall:     0.8580\n",
      "F1:         0.8692\n",
      "Training - Epoch [14/32]\n",
      "Loss: 0.1655\n",
      "Accuracy:   0.8770\n",
      "Precision:  0.8738\n",
      "Recall:     0.8798\n",
      "F1 Score:   0.8768\n",
      "Validation - Epoch [14/32]\n",
      "Loss: 0.1655\n",
      "Accuracy:   0.8684\n",
      "Precision:  0.8812\n",
      "Recall:     0.8565\n",
      "F1:         0.8687\n",
      "Training - Epoch [15/32]\n",
      "Loss: 0.1624\n",
      "Accuracy:   0.8785\n",
      "Precision:  0.8751\n",
      "Recall:     0.8817\n",
      "F1 Score:   0.8784\n",
      "Validation - Epoch [15/32]\n",
      "Loss: 0.1624\n",
      "Accuracy:   0.8684\n",
      "Precision:  0.8814\n",
      "Recall:     0.8562\n",
      "F1:         0.8686\n",
      "Training - Epoch [16/32]\n",
      "Loss: 0.1597\n",
      "Accuracy:   0.8802\n",
      "Precision:  0.8764\n",
      "Recall:     0.8837\n",
      "F1 Score:   0.8801\n",
      "Validation - Epoch [16/32]\n",
      "Loss: 0.1597\n",
      "Accuracy:   0.8691\n",
      "Precision:  0.8816\n",
      "Recall:     0.8575\n",
      "F1:         0.8694\n",
      "Training - Epoch [17/32]\n",
      "Loss: 0.1573\n",
      "Accuracy:   0.8817\n",
      "Precision:  0.8782\n",
      "Recall:     0.8849\n",
      "F1 Score:   0.8815\n",
      "Validation - Epoch [17/32]\n",
      "Loss: 0.1573\n",
      "Accuracy:   0.8696\n",
      "Precision:  0.8823\n",
      "Recall:     0.8578\n",
      "F1:         0.8699\n",
      "Training - Epoch [18/32]\n",
      "Loss: 0.1552\n",
      "Accuracy:   0.8831\n",
      "Precision:  0.8794\n",
      "Recall:     0.8866\n",
      "F1 Score:   0.8830\n",
      "Validation - Epoch [18/32]\n",
      "Loss: 0.1552\n",
      "Accuracy:   0.8692\n",
      "Precision:  0.8814\n",
      "Recall:     0.8580\n",
      "F1:         0.8696\n",
      "Training - Epoch [19/32]\n",
      "Loss: 0.1531\n",
      "Accuracy:   0.8845\n",
      "Precision:  0.8806\n",
      "Recall:     0.8883\n",
      "F1 Score:   0.8844\n",
      "Validation - Epoch [19/32]\n",
      "Loss: 0.1531\n",
      "Accuracy:   0.8691\n",
      "Precision:  0.8814\n",
      "Recall:     0.8578\n",
      "F1:         0.8694\n",
      "Training - Epoch [20/32]\n",
      "Loss: 0.1510\n",
      "Accuracy:   0.8857\n",
      "Precision:  0.8813\n",
      "Recall:     0.8902\n",
      "F1 Score:   0.8857\n",
      "Validation - Epoch [20/32]\n",
      "Loss: 0.1510\n",
      "Accuracy:   0.8695\n",
      "Precision:  0.8817\n",
      "Recall:     0.8583\n",
      "F1:         0.8698\n",
      "Training - Epoch [21/32]\n",
      "Loss: 0.1488\n",
      "Accuracy:   0.8877\n",
      "Precision:  0.8830\n",
      "Recall:     0.8925\n",
      "F1 Score:   0.8877\n",
      "Validation - Epoch [21/32]\n",
      "Loss: 0.1488\n",
      "Accuracy:   0.8697\n",
      "Precision:  0.8819\n",
      "Recall:     0.8586\n",
      "F1:         0.8701\n",
      "Training - Epoch [22/32]\n",
      "Loss: 0.1463\n",
      "Accuracy:   0.8891\n",
      "Precision:  0.8846\n",
      "Recall:     0.8936\n",
      "F1 Score:   0.8891\n",
      "Validation - Epoch [22/32]\n",
      "Loss: 0.1463\n",
      "Accuracy:   0.8687\n",
      "Precision:  0.8811\n",
      "Recall:     0.8573\n",
      "F1:         0.8690\n",
      "Training - Epoch [23/32]\n",
      "Loss: 0.1436\n",
      "Accuracy:   0.8895\n",
      "Precision:  0.8850\n",
      "Recall:     0.8941\n",
      "F1 Score:   0.8895\n",
      "Validation - Epoch [23/32]\n",
      "Loss: 0.1436\n",
      "Accuracy:   0.8683\n",
      "Precision:  0.8799\n",
      "Recall:     0.8578\n",
      "F1:         0.8687\n",
      "Training - Epoch [24/32]\n",
      "Loss: 0.1409\n",
      "Accuracy:   0.8912\n",
      "Precision:  0.8866\n",
      "Recall:     0.8960\n",
      "F1 Score:   0.8913\n",
      "Validation - Epoch [24/32]\n",
      "Loss: 0.1409\n",
      "Accuracy:   0.8684\n",
      "Precision:  0.8796\n",
      "Recall:     0.8586\n",
      "F1:         0.8689\n",
      "Training - Epoch [25/32]\n",
      "Loss: 0.1380\n",
      "Accuracy:   0.8930\n",
      "Precision:  0.8883\n",
      "Recall:     0.8977\n",
      "F1 Score:   0.8930\n",
      "Validation - Epoch [25/32]\n",
      "Loss: 0.1380\n",
      "Accuracy:   0.8679\n",
      "Precision:  0.8780\n",
      "Recall:     0.8594\n",
      "F1:         0.8686\n",
      "Training - Epoch [26/32]\n",
      "Loss: 0.1349\n",
      "Accuracy:   0.8946\n",
      "Precision:  0.8901\n",
      "Recall:     0.8992\n",
      "F1 Score:   0.8946\n",
      "Validation - Epoch [26/32]\n",
      "Loss: 0.1349\n",
      "Accuracy:   0.8685\n",
      "Precision:  0.8786\n",
      "Recall:     0.8601\n",
      "F1:         0.8693\n",
      "Training - Epoch [27/32]\n",
      "Loss: 0.1318\n",
      "Accuracy:   0.8963\n",
      "Precision:  0.8919\n",
      "Recall:     0.9007\n",
      "F1 Score:   0.8963\n",
      "Validation - Epoch [27/32]\n",
      "Loss: 0.1318\n",
      "Accuracy:   0.8683\n",
      "Precision:  0.8781\n",
      "Recall:     0.8601\n",
      "F1:         0.8690\n",
      "Training - Epoch [28/32]\n",
      "Loss: 0.1288\n",
      "Accuracy:   0.8979\n",
      "Precision:  0.8941\n",
      "Recall:     0.9016\n",
      "F1 Score:   0.8978\n",
      "Validation - Epoch [28/32]\n",
      "Loss: 0.1288\n",
      "Accuracy:   0.8672\n",
      "Precision:  0.8766\n",
      "Recall:     0.8596\n",
      "F1:         0.8680\n",
      "Training - Epoch [29/32]\n",
      "Loss: 0.1258\n",
      "Accuracy:   0.9000\n",
      "Precision:  0.8962\n",
      "Recall:     0.9036\n",
      "F1 Score:   0.8999\n",
      "Validation - Epoch [29/32]\n",
      "Loss: 0.1258\n",
      "Accuracy:   0.8669\n",
      "Precision:  0.8756\n",
      "Recall:     0.8604\n",
      "F1:         0.8679\n",
      "Training - Epoch [30/32]\n",
      "Loss: 0.1229\n",
      "Accuracy:   0.9017\n",
      "Precision:  0.8984\n",
      "Recall:     0.9047\n",
      "F1 Score:   0.9015\n",
      "Validation - Epoch [30/32]\n",
      "Loss: 0.1229\n",
      "Accuracy:   0.8668\n",
      "Precision:  0.8741\n",
      "Recall:     0.8620\n",
      "F1:         0.8680\n",
      "Training - Epoch [31/32]\n",
      "Loss: 0.1200\n",
      "Accuracy:   0.9040\n",
      "Precision:  0.9010\n",
      "Recall:     0.9066\n",
      "F1 Score:   0.9038\n",
      "Validation - Epoch [31/32]\n",
      "Loss: 0.1200\n",
      "Accuracy:   0.8667\n",
      "Precision:  0.8731\n",
      "Recall:     0.8630\n",
      "F1:         0.8680\n",
      "Training - Epoch [32/32]\n",
      "Loss: 0.1169\n",
      "Accuracy:   0.9064\n",
      "Precision:  0.9042\n",
      "Recall:     0.9082\n",
      "F1 Score:   0.9062\n",
      "Validation - Epoch [32/32]\n",
      "Loss: 0.1169\n",
      "Accuracy:   0.8657\n",
      "Precision:  0.8731\n",
      "Recall:     0.8609\n",
      "F1:         0.8670\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "\n",
    "    for features, labels in train_loader:\n",
    "        features, labels = features.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(features)\n",
    "\n",
    "        # Ensure labels are float\n",
    "        labels = labels.view(-1, 1).float()\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Convert outputs to binary predictions\n",
    "        preds = torch.round(torch.sigmoid(outputs))\n",
    "        predictions.extend(preds.detach().cpu().numpy())\n",
    "        true_labels.extend(labels.detach().cpu().numpy())\n",
    "\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(true_labels, predictions, average='binary')\n",
    "\n",
    "    print(f'Training - Epoch [{epoch+1}/{num_epochs}]')\n",
    "    print(f'Loss: {loss.item():.4f}')  # Print loss for the last batch\n",
    "    print(f'Accuracy:   {accuracy:.4f}')\n",
    "    print(f'Precision:  {precision:.4f}')\n",
    "    print(f'Recall:     {recall:.4f}')\n",
    "    print(f'F1 Score:   {f1:.4f}')\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    val_predictions = []\n",
    "    val_true_labels = []\n",
    "\n",
    "    for features, labels in val_loader:\n",
    "        features, labels = features.to(device), labels.to(device)\n",
    "        val_outputs = model(features)\n",
    "\n",
    "        # Ensure labels are float\n",
    "        labels = labels.view(-1, 1).float()\n",
    "\n",
    "        # Convert outputs to binary predictions\n",
    "        val_preds = torch.round(torch.sigmoid(val_outputs))\n",
    "        val_predictions.extend(val_preds.detach().cpu().numpy())\n",
    "        val_true_labels.extend(labels.detach().cpu().numpy())\n",
    "\n",
    "    val_accuracy = accuracy_score(val_true_labels, val_predictions)\n",
    "    val_precision, val_recall, val_f1, _ = precision_recall_fscore_support(val_true_labels, val_predictions, average='binary')\n",
    "\n",
    "    # Print validation metrics\n",
    "    print(f'Validation - Epoch [{epoch+1}/{num_epochs}]')\n",
    "    print(f'Loss: {loss.item():.4f}')  # Print loss for the last batch\n",
    "    print(f'Accuracy:   {val_accuracy:.4f}')\n",
    "    print(f'Precision:  {val_precision:.4f}')\n",
    "    print(f'Recall:     {val_recall:.4f}')\n",
    "    print(f'F1:         {val_f1:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96da80fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/miso/miniconda3/envs/nlp/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-06-23 02:52:56.740495: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-06-23 02:52:57.012949: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-23 02:52:57.671178: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens:\n",
      "{'input_ids': tensor([[ 101, 2023, 2003, 1037, 7099, 6251, 1012,  102]], device='cuda:0'), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}\n",
      "\n",
      "Batch Tokens:\n",
      "{'input_ids': tensor([[ 101, 2023, 2003, 1996, 2034, 6251, 1012,  102],\n",
      "        [ 101, 2023, 2003, 1996, 2117, 6251, 1012,  102]], device='cuda:0'), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, pipeline\n",
    "\n",
    "# Load a pre-trained tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Tokenize a single sentence\n",
    "text = \"This is a sample sentence.\"\n",
    "tokens = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "# Convert the tokens to a format suitable for GPU processing\n",
    "tokens = {k: v.to('cuda') for k, v in tokens.items()}\n",
    "\n",
    "print(\"Tokens:\")\n",
    "print(tokens)\n",
    "\n",
    "# Tokenize multiple sentences\n",
    "texts = [\"This is the first sentence.\", \"This is the second sentence.\"]\n",
    "batch_tokens = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "# Convert the batch tokens to a format suitable for GPU processing\n",
    "batch_tokens = {k: v.to('cuda') for k, v in batch_tokens.items()}\n",
    "\n",
    "print(\"\\nBatch Tokens:\")\n",
    "print(batch_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "249a3da3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text\n",
      "0  the half-ling book one in the fall of igneeria...\n",
      "1  isbn : 1492913731 isbn-13 : 978-1492913733 for...\n",
      "2    i wish i had a better answer to that question .\n",
      "3  starlings , new york is not the place youd exp...\n",
      "4  its a small quiet town , the kind where everyo...\n"
     ]
    }
   ],
   "source": [
    "import pyarrow as pa\n",
    "import pyarrow.ipc as ipc\n",
    "import json\n",
    "\n",
    "with pa.memory_map('dataset.arrow', 'r') as source:\n",
    "        reader = ipc.open_stream(source)\n",
    "        table = reader.read_all()\n",
    "\n",
    "# Convert to Pandas DataFrame if needed\n",
    "df = table.to_pandas()\n",
    "print(df.head())\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f254874b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the half-ling book one in the fall of igneeria...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>isbn : 1492913731 isbn-13 : 978-1492913733 for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i wish i had a better answer to that question .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>starlings , new york is not the place youd exp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>its a small quiet town , the kind where everyo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74004223</th>\n",
       "      <td>he crawled up my body to kiss me .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74004224</th>\n",
       "      <td>`` and you 're the best husband ever .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74004225</th>\n",
       "      <td>now , make me scream your name again . ''</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74004226</th>\n",
       "      <td>no regrets in love .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74004227</th>\n",
       "      <td>ever .</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>74004228 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       text\n",
       "0         the half-ling book one in the fall of igneeria...\n",
       "1         isbn : 1492913731 isbn-13 : 978-1492913733 for...\n",
       "2           i wish i had a better answer to that question .\n",
       "3         starlings , new york is not the place youd exp...\n",
       "4         its a small quiet town , the kind where everyo...\n",
       "...                                                     ...\n",
       "74004223                 he crawled up my body to kiss me .\n",
       "74004224             `` and you 're the best husband ever .\n",
       "74004225          now , make me scream your name again . ''\n",
       "74004226                               no regrets in love .\n",
       "74004227                                             ever .\n",
       "\n",
       "[74004228 rows x 1 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87537a13",
   "metadata": {},
   "source": [
    "GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6d328af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing:   0%|          | 2310/1156317 [00:16<2:19:54, 137.47it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(texts), batch_size), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTokenizing\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     18\u001b[0m     batch_texts \u001b[38;5;241m=\u001b[39m texts[i:i\u001b[38;5;241m+\u001b[39mbatch_size]\n\u001b[0;32m---> 19\u001b[0m     batch_tokens \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_encode_plus\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_texts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmax_length\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m     batch_tokens \u001b[38;5;241m=\u001b[39m {k: v\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m batch_tokens\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m     21\u001b[0m     tokens\u001b[38;5;241m.\u001b[39mappend(batch_tokens)\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:3160\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   3150\u001b[0m \u001b[38;5;66;03m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[1;32m   3151\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[1;32m   3152\u001b[0m     padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[1;32m   3153\u001b[0m     truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3157\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3158\u001b[0m )\n\u001b[0;32m-> 3160\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3161\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3162\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3163\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3164\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3165\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3166\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3167\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3168\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3169\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3170\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3171\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3172\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3173\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3176\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3177\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3178\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.11/site-packages/transformers/tokenization_utils_fast.py:523\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose)\u001b[0m\n\u001b[1;32m    511\u001b[0m encodings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tokenizer\u001b[38;5;241m.\u001b[39mencode_batch(\n\u001b[1;32m    512\u001b[0m     batch_text_or_text_pairs,\n\u001b[1;32m    513\u001b[0m     add_special_tokens\u001b[38;5;241m=\u001b[39madd_special_tokens,\n\u001b[1;32m    514\u001b[0m     is_pretokenized\u001b[38;5;241m=\u001b[39mis_split_into_words,\n\u001b[1;32m    515\u001b[0m )\n\u001b[1;32m    517\u001b[0m \u001b[38;5;66;03m# Convert encoding to dict\u001b[39;00m\n\u001b[1;32m    518\u001b[0m \u001b[38;5;66;03m# `Tokens` has type: Tuple[\u001b[39;00m\n\u001b[1;32m    519\u001b[0m \u001b[38;5;66;03m#                       List[Dict[str, List[List[int]]]] or List[Dict[str, 2D-Tensor]],\u001b[39;00m\n\u001b[1;32m    520\u001b[0m \u001b[38;5;66;03m#                       List[EncodingFast]\u001b[39;00m\n\u001b[1;32m    521\u001b[0m \u001b[38;5;66;03m#                    ]\u001b[39;00m\n\u001b[1;32m    522\u001b[0m \u001b[38;5;66;03m# with nested dimensions corresponding to batch, overflows, sequence length\u001b[39;00m\n\u001b[0;32m--> 523\u001b[0m tokens_and_encodings \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_encoding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    525\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    528\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    529\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    530\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    531\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    532\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    533\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    534\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mencodings\u001b[49m\n\u001b[1;32m    535\u001b[0m \u001b[43m\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;66;03m# Convert the output to have dict[list] from list[dict] and remove the additional overflows dimension\u001b[39;00m\n\u001b[1;32m    538\u001b[0m \u001b[38;5;66;03m# From (variable) shape (batch, overflows, sequence length) to ~ (batch * overflows, sequence length)\u001b[39;00m\n\u001b[1;32m    539\u001b[0m \u001b[38;5;66;03m# (we say ~ because the number of overflow varies with the example in the batch)\u001b[39;00m\n\u001b[1;32m    540\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    541\u001b[0m \u001b[38;5;66;03m# To match each overflowing sample with the original sample in the batch\u001b[39;00m\n\u001b[1;32m    542\u001b[0m \u001b[38;5;66;03m# we add an overflow_to_sample_mapping array (see below)\u001b[39;00m\n\u001b[1;32m    543\u001b[0m sanitized_tokens \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp/lib/python3.11/site-packages/transformers/tokenization_utils_fast.py:523\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    511\u001b[0m encodings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tokenizer\u001b[38;5;241m.\u001b[39mencode_batch(\n\u001b[1;32m    512\u001b[0m     batch_text_or_text_pairs,\n\u001b[1;32m    513\u001b[0m     add_special_tokens\u001b[38;5;241m=\u001b[39madd_special_tokens,\n\u001b[1;32m    514\u001b[0m     is_pretokenized\u001b[38;5;241m=\u001b[39mis_split_into_words,\n\u001b[1;32m    515\u001b[0m )\n\u001b[1;32m    517\u001b[0m \u001b[38;5;66;03m# Convert encoding to dict\u001b[39;00m\n\u001b[1;32m    518\u001b[0m \u001b[38;5;66;03m# `Tokens` has type: Tuple[\u001b[39;00m\n\u001b[1;32m    519\u001b[0m \u001b[38;5;66;03m#                       List[Dict[str, List[List[int]]]] or List[Dict[str, 2D-Tensor]],\u001b[39;00m\n\u001b[1;32m    520\u001b[0m \u001b[38;5;66;03m#                       List[EncodingFast]\u001b[39;00m\n\u001b[1;32m    521\u001b[0m \u001b[38;5;66;03m#                    ]\u001b[39;00m\n\u001b[1;32m    522\u001b[0m \u001b[38;5;66;03m# with nested dimensions corresponding to batch, overflows, sequence length\u001b[39;00m\n\u001b[0;32m--> 523\u001b[0m tokens_and_encodings \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    524\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_encoding(\n\u001b[1;32m    525\u001b[0m         encoding\u001b[38;5;241m=\u001b[39mencoding,\n\u001b[1;32m    526\u001b[0m         return_token_type_ids\u001b[38;5;241m=\u001b[39mreturn_token_type_ids,\n\u001b[1;32m    527\u001b[0m         return_attention_mask\u001b[38;5;241m=\u001b[39mreturn_attention_mask,\n\u001b[1;32m    528\u001b[0m         return_overflowing_tokens\u001b[38;5;241m=\u001b[39mreturn_overflowing_tokens,\n\u001b[1;32m    529\u001b[0m         return_special_tokens_mask\u001b[38;5;241m=\u001b[39mreturn_special_tokens_mask,\n\u001b[1;32m    530\u001b[0m         return_offsets_mapping\u001b[38;5;241m=\u001b[39mreturn_offsets_mapping,\n\u001b[1;32m    531\u001b[0m         return_length\u001b[38;5;241m=\u001b[39mreturn_length,\n\u001b[1;32m    532\u001b[0m         verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[1;32m    533\u001b[0m     )\n\u001b[1;32m    534\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m encoding \u001b[38;5;129;01min\u001b[39;00m encodings\n\u001b[1;32m    535\u001b[0m ]\n\u001b[1;32m    537\u001b[0m \u001b[38;5;66;03m# Convert the output to have dict[list] from list[dict] and remove the additional overflows dimension\u001b[39;00m\n\u001b[1;32m    538\u001b[0m \u001b[38;5;66;03m# From (variable) shape (batch, overflows, sequence length) to ~ (batch * overflows, sequence length)\u001b[39;00m\n\u001b[1;32m    539\u001b[0m \u001b[38;5;66;03m# (we say ~ because the number of overflow varies with the example in the batch)\u001b[39;00m\n\u001b[1;32m    540\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    541\u001b[0m \u001b[38;5;66;03m# To match each overflowing sample with the original sample in the batch\u001b[39;00m\n\u001b[1;32m    542\u001b[0m \u001b[38;5;66;03m# we add an overflow_to_sample_mapping array (see below)\u001b[39;00m\n\u001b[1;32m    543\u001b[0m sanitized_tokens \u001b[38;5;241m=\u001b[39m {}\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load a pre-trained BERT tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-large-uncased\")\n",
    "model = AutoModel.from_pretrained(\"bert-large-uncased\")\n",
    "\n",
    "# Move the model to the GPU if available\n",
    "device = torch.device(\"cuda\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Tokenize the text with a progress bar\n",
    "texts = df['text'].tolist()\n",
    "tokens = []\n",
    "for text in tqdm(texts, desc=\"Tokenizing\"):\n",
    "    token = tokenizer.encode_plus(text, return_tensors=\"pt\", padding='max_length', truncation=True, max_length=512)\n",
    "    token = {k: v.to(device) for k, v in token.items()}\n",
    "    tokens.append(token)\n",
    "\n",
    "# Embed the tokens using the BERT model\n",
    "with torch.no_grad():\n",
    "    outputs = [model(**token) for token in tqdm(tokens, desc=\"Embedding\")]\n",
    "\n",
    "# Get the embeddings for the [CLS] token (first token)\n",
    "embeddings = [output.last_hidden_state[:, 0, :] for output in outputs]\n",
    "\n",
    "# Print the embeddings\n",
    "for embedding in embeddings:\n",
    "    print(embedding)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484e18ab",
   "metadata": {},
   "source": [
    "CPU"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
